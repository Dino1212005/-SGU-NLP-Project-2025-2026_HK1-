{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38298a54",
   "metadata": {},
   "source": [
    "# 1. Chuẩn bị dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4359fd",
   "metadata": {},
   "source": [
    "# Cài đặt\n",
    "- pip install spacy\n",
    "- python -m spacy download en_core_web_sm\n",
    "- python -m spacy download de_core_news_sm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6936951",
   "metadata": {},
   "source": [
    "# 2. Tokenization – dùng Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72dab85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# English tokenizer\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "def tokenizer_en(text):\n",
    "    return spacy_en.tokenizer(text)\n",
    "\n",
    "# German tokenizer\n",
    "spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "def tokenizer_de(text):\n",
    "    return spacy_de.tokenizer(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d827fb35",
   "metadata": {},
   "source": [
    "# 3.Load EN–DE từ file .gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1a35410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def load_parallel_corpus(en_file, de_file):\n",
    "    sentences_en = []\n",
    "    sentences_de = []\n",
    "\n",
    "    with gzip.open(en_file, 'rt', encoding='utf-8') as f_en, \\\n",
    "         gzip.open(de_file, 'rt', encoding='utf-8') as f_de:\n",
    "\n",
    "        for en_line, de_line in zip(f_en, f_de):\n",
    "            en = en_line.strip()\n",
    "            de = de_line.strip()\n",
    "            sentences_en.append(en)\n",
    "            sentences_de.append(de)\n",
    "\n",
    "    return sentences_en, sentences_de\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718dac22",
   "metadata": {},
   "source": [
    "# 3.1 Load train / val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1d05515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two young, White males are outside near many bushes.\n",
      "Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n"
     ]
    }
   ],
   "source": [
    "train_en, train_de = load_parallel_corpus(\"train.en.gz\", \"train.de.gz\")\n",
    "val_en, val_de = load_parallel_corpus(\"val.en.gz\", \"val.de.gz\")\n",
    "# Kiểm tra đã load được chưa\n",
    "print(train_en[0])\n",
    "print(train_de[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3550faaf",
   "metadata": {},
   "source": [
    "# 4. Xây dựng từ điển(Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6786cd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "special_tokens = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "\n",
    "def build_vocab(sentences, tokenizer, max_words=10000):\n",
    "    counter = Counter()\n",
    "    for sent in sentences:\n",
    "        tokens = [t.text.lower() for t in tokenizer(sent)]\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # Chọn 10000 từ phổ biến nhất\n",
    "    most_common = counter.most_common(max_words - len(special_tokens))\n",
    "\n",
    "    vocab = special_tokens + [w for w, _ in most_common]\n",
    "    stoi = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "    return vocab, stoi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361cf2d6",
   "metadata": {},
   "source": [
    "# 4.1 Build vocab EN & DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7cda0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary EN: 9797\n",
      "Vocabulary DE: 10000\n"
     ]
    }
   ],
   "source": [
    "vocab_en, stoi_en = build_vocab(train_en, tokenizer_en)\n",
    "vocab_de, stoi_de = build_vocab(train_de, tokenizer_de)\n",
    "\n",
    "print(\"Vocabulary EN:\", len(vocab_en))\n",
    "print(\"Vocabulary DE:\", len(vocab_de))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6db1e85",
   "metadata": {},
   "source": [
    "# 5. Hàm convert câu → id + thêm <sos> <eos>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dcb4c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(sentence, tokenizer, stoi):\n",
    "    tokens = [\"<sos>\"] + [t.text.lower() for t in tokenizer(sentence)] + [\"<eos>\"]\n",
    "    return [stoi.get(tok, stoi[\"<unk>\"]) for tok in tokens]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf9b38",
   "metadata": {},
   "source": [
    "# 5.1 Tạo dataset dạng list of (tensor_en, tensor_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d64b4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def make_dataset(en_sentences, de_sentences, tokenizer_en, tokenizer_de, stoi_en, stoi_de):\n",
    "    data = []\n",
    "    for en, de in zip(en_sentences, de_sentences):\n",
    "        en_ids = torch.tensor(numericalize(en, tokenizer_en, stoi_en))\n",
    "        de_ids = torch.tensor(numericalize(de, tokenizer_de, stoi_de))\n",
    "        data.append((en_ids, de_ids))\n",
    "    return data\n",
    "\n",
    "train_dataset = make_dataset(train_en, train_de, tokenizer_en, tokenizer_de, stoi_en, stoi_de)\n",
    "val_dataset   = make_dataset(val_en, val_de, tokenizer_en, tokenizer_de, stoi_en, stoi_de)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8f1a17",
   "metadata": {},
   "source": [
    "# 5.2 collate_fn (chuẩn cho padding + packing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3bda5fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "PAD_IDX_EN = stoi_en[\"<pad>\"]\n",
    "PAD_IDX_DE = stoi_de[\"<pad>\"]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch = [(en_ids, de_ids), ...]\n",
    "    en_list = [item[0] for item in batch]\n",
    "    de_list = [item[1] for item in batch]\n",
    "\n",
    "    # Lấy độ dài gốc\n",
    "    en_lengths = torch.tensor([len(x) for x in en_list])\n",
    "    de_lengths = torch.tensor([len(x) for x in de_list])\n",
    "\n",
    "    # Sắp xếp theo độ dài giảm dần (required for pack_padded_sequence)\n",
    "    en_lengths, sort_idx = en_lengths.sort(descending=True)\n",
    "    en_list = [en_list[i] for i in sort_idx]\n",
    "    de_list = [de_list[i] for i in sort_idx]\n",
    "    de_lengths = de_lengths[sort_idx]\n",
    "\n",
    "    # Padding\n",
    "    en_padded = pad_sequence(en_list, batch_first=True, padding_value=PAD_IDX_EN)\n",
    "    de_padded = pad_sequence(de_list, batch_first=True, padding_value=PAD_IDX_DE)\n",
    "\n",
    "    return en_padded, en_lengths, de_padded, de_lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21053afa",
   "metadata": {},
   "source": [
    "# 6. DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d9a3e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    "    \n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb14bc28",
   "metadata": {},
   "source": [
    "Cách dùng trong LSTM Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a81c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "def forward(self, src, src_lengths):\n",
    "    # src shape: (batch, seq_len)\n",
    "    embedded = self.embedding(src)\n",
    "\n",
    "    packed = pack_padded_sequence(\n",
    "        embedded,\n",
    "        src_lengths.cpu(),\n",
    "        batch_first=True,\n",
    "        enforce_sorted=True\n",
    "    )\n",
    "\n",
    "    outputs, hidden = self.lstm(packed)\n",
    "\n",
    "    outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
    "\n",
    "    return outputs, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a956d4f",
   "metadata": {},
   "source": [
    "# 7. Xây dựng mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b00698",
   "metadata": {},
   "source": [
    "## 7.1 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "14bd0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, hidden_size=512, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=stoi_en[\"<pad>\"])\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, src, src_lengths):\n",
    "        # src: (batch, seq_len)\n",
    "        embedded = self.embedding(src)  # (B, L, E)\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=True\n",
    "        )\n",
    "\n",
    "        outputs, (h_n, c_n) = self.lstm(packed)\n",
    "\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "\n",
    "        return outputs, (h_n, c_n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a03876",
   "metadata": {},
   "source": [
    "## 7.2 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ca0f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, hidden_size=512, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=stoi_de[\"<pad>\"])\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_token, hidden):\n",
    "        # input_token: (batch,) 1 token tại bước t\n",
    "        # hidden = (h, c)\n",
    "\n",
    "        embedded = self.embedding(input_token).unsqueeze(1)  # (B,1,E)\n",
    "\n",
    "        output, hidden = self.lstm(embedded, hidden)  # output: (B,1,H)\n",
    "\n",
    "        logits = self.fc(output.squeeze(1))  # (B, vocab)\n",
    "\n",
    "        return logits, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33bcba7",
   "metadata": {},
   "source": [
    "## 7.3 Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a6c33fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, teacher_forcing_ratio=0.5):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "    def forward(self, src, src_lengths, trg):\n",
    "        # src: (B, Ls)\n",
    "        # trg: (B, Lt)\n",
    "        batch_size, trg_len = trg.size()\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n",
    "\n",
    "        # ---- Encoder ----\n",
    "        _, hidden = self.encoder(src, src_lengths)\n",
    "\n",
    "        # token đầu tiên cho decoder = <sos>\n",
    "        input_token = trg[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            logits, hidden = self.decoder(input_token, hidden)\n",
    "            outputs[:, t] = logits\n",
    "\n",
    "            # chọn token dự đoán\n",
    "            predicted = logits.argmax(dim=1)\n",
    "\n",
    "            # teacher forcing ?\n",
    "            if random.random() < self.teacher_forcing_ratio:\n",
    "                input_token = trg[:, t]     # dùng ground truth\n",
    "            else:\n",
    "                input_token = predicted     # dùng dự đoán\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9a0030",
   "metadata": {},
   "source": [
    "## 7.4 Khởi tạo mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e51fe86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(\n",
    "    vocab_size=len(vocab_en),\n",
    "    embed_dim=512,\n",
    "    hidden_size=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    vocab_size=len(vocab_de),\n",
    "    embed_dim=512,\n",
    "    hidden_size=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44822159",
   "metadata": {},
   "source": [
    "# 8. Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1d66af42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train loss: 4.7478 | Val loss: 3.6906 (best -> saved) | Time: 1025.9s\n",
      "Epoch 02 | Train loss: 3.8015 | Val loss: 3.0989 (best -> saved) | Time: 1157.1s\n",
      "Epoch 03 | Train loss: 3.3495 | Val loss: 2.7848 (best -> saved) | Time: 1112.8s\n",
      "Epoch 04 | Train loss: 2.9877 | Val loss: 2.5988 (best -> saved) | Time: 1093.9s\n",
      "Epoch 05 | Train loss: 2.6939 | Val loss: 2.4614 (best -> saved) | Time: 1038.3s\n",
      "Epoch 06 | Train loss: 2.4429 | Val loss: 2.3832 (best -> saved) | Time: 1090.8s\n",
      "Epoch 07 | Train loss: 2.2058 | Val loss: 2.3268 (best -> saved) | Time: 1124.3s\n",
      "Epoch 08 | Train loss: 2.0009 | Val loss: 2.3069 (best -> saved) | Time: 1327.9s\n",
      "Epoch 09 | Train loss: 1.8283 | Val loss: 2.2997 (best -> saved) | Time: 1310.3s\n",
      "Epoch 10 | Train loss: 1.6408 | Val loss: 2.3131 | Time: 1203.4s\n",
      "Training finished. Best val loss: 2.2997\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Config\n",
    "LR = 0.001\n",
    "NUM_EPOCHS = 10       # bạn có thể đặt 10-20\n",
    "PATIENCE = 3            # early stopping nếu val_loss không giảm sau 3 epoch\n",
    "CLIP = 1.0              # grad clipping\n",
    "USE_SCHEDULER = True    # nếu muốn dùng ReduceLROnPlateau\n",
    "\n",
    "# Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX_DE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Optional scheduler\n",
    "try:\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=1, verbose=True\n",
    "    )\n",
    "except TypeError:\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=1\n",
    "    )\n",
    "# Helper: evaluation on validation set (no teacher forcing)\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    # Turn off teacher forcing during validation (full autoregressive)\n",
    "    prev_tf = getattr(model, \"teacher_forcing_ratio\", 0.0)\n",
    "    model.teacher_forcing_ratio = 1.0\n",
    "\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for src, src_lengths, trg, trg_lengths in val_loader:\n",
    "            src = src.to(device)\n",
    "            src_lengths = src_lengths.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            outputs = model(src, src_lengths, trg)  # (B, T, V)\n",
    "            vocab_size = outputs.size(-1)\n",
    "\n",
    "            # ignore the first token (<sos>) when computing loss\n",
    "            pred = outputs[:, 1:, :].contiguous().view(-1, vocab_size)   # (B*(T-1), V)\n",
    "            target = trg[:, 1:].contiguous().view(-1)                    # (B*(T-1))\n",
    "\n",
    "            loss = criterion(pred, target)\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "    model.teacher_forcing_ratio = prev_tf\n",
    "    return total_loss / (n_batches if n_batches > 0 else 1)\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for src, src_lengths, trg, trg_lengths in train_loader:\n",
    "        src = src.to(device, non_blocking=True)\n",
    "\n",
    "        src_lengths = src_lengths.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(src, src_lengths, trg)  # (B, T, V)\n",
    "        vocab_size = outputs.size(-1)\n",
    "\n",
    "        # shift: ignore <sos> token in loss\n",
    "        pred = outputs[:, 1:, :].contiguous().view(-1, vocab_size)  # (B*(T-1), V)\n",
    "        target = trg[:, 1:].contiguous().view(-1)                   # (B*(T-1))\n",
    "\n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_train_loss = train_loss / (n_batches if n_batches > 0 else 1)\n",
    "    avg_val_loss = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "\n",
    "    # Scheduler step on validation loss\n",
    "    if USE_SCHEDULER:\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        epochs_no_improve = 0\n",
    "        best_note = \" (best -> saved)\"\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        best_note = \"\"\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Epoch {epoch:02d} | Train loss: {avg_train_loss:.4f} | Val loss: {avg_val_loss:.4f}{best_note} | Time: {elapsed:.1f}s\")\n",
    "\n",
    "    # Early stopping\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f\"Early stopping triggered. No improvement for {PATIENCE} epochs.\")\n",
    "        break\n",
    "\n",
    "print(\"Training finished. Best val loss: {:.4f}\".format(best_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3d712",
   "metadata": {},
   "source": [
    "# 9. Dự đoán (Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4bff483b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INFERENCE EXAMPLES (Greedy Decoding)\n",
      "============================================================\n",
      "EN: Hello, how are you?\n",
      "DE: es werden <unk> <unk>.\n",
      "\n",
      "EN: What is your name?\n",
      "DE: <unk> ist ein <unk> <unk>.\n",
      "\n",
      "EN: The weather is nice today.\n",
      "DE: der <unk> <unk> <unk>.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Helper: Build reverse vocab (id -> token)\n",
    "def build_itos(vocab):\n",
    "    \"\"\"Index to string mapping\"\"\"\n",
    "    return {i: w for i, w in enumerate(vocab)}\n",
    "\n",
    "itos_de = build_itos(vocab_de)\n",
    "\n",
    "# Helper: Detokenize German sentence\n",
    "def detokenize_de(tokens):\n",
    "    \"\"\"\n",
    "    Ghép tokens lại thành câu (detokenize)\n",
    "    Đơn giản: join với space, sau đó xử lý dấu câu và contractions\n",
    "    \"\"\"\n",
    "    text = \" \".join(tokens)\n",
    "    # Xóa space trước dấu câu\n",
    "    text = text.replace(\" .\", \".\").replace(\" ,\", \",\").replace(\" !\", \"!\").replace(\" ?\", \"?\")\n",
    "    return text.strip()\n",
    "\n",
    "def translate(sentence: str, model, device, tokenizer_en, stoi_en, itos_de, stoi_de, \n",
    "              max_length=50, beam_width=1) -> str:\n",
    "    \"\"\"\n",
    "    Dịch câu tiếng Anh sang tiếng Đức (Greedy Decoding).\n",
    "    \n",
    "    Args:\n",
    "        sentence: Input English sentence\n",
    "        model: Seq2Seq model\n",
    "        device: torch device (cpu/cuda)\n",
    "        tokenizer_en: Spacy English tokenizer\n",
    "        stoi_en: English string-to-index vocab\n",
    "        itos_de: German index-to-string vocab\n",
    "        max_length: Maximum output length\n",
    "        beam_width: 1 for greedy, >1 for beam search (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Translated German sentence as string\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # ---- 1. Tokenize + Numericalize input (English) ----\n",
    "    tokens_en = [t.text.lower() for t in tokenizer_en(sentence)]\n",
    "    input_ids = [stoi_en.get(\"<sos>\", 1)] + [stoi_en.get(tok, stoi_en[\"<unk>\"]) for tok in tokens_en] + [stoi_en.get(\"<eos>\", 3)]\n",
    "    src_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)  # (1, seq_len)\n",
    "    src_length = torch.tensor([len(input_ids)], dtype=torch.long).to(device)  # (1,)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # ---- 2. Encoder ----\n",
    "        _, hidden = model.encoder(src_tensor, src_length)\n",
    "        \n",
    "        # ---- 3. Decoder (Greedy) ----\n",
    "        output_ids = [stoi_de[\"<sos>\"]]  # Start with <sos>\n",
    "        input_token = torch.tensor([stoi_de[\"<sos>\"]], dtype=torch.long).to(device)  # (1,)\n",
    "        \n",
    "        for t in range(1, max_length):\n",
    "            logits, hidden = model.decoder(input_token, hidden)  # (1, vocab_size)\n",
    "            \n",
    "            # Greedy: chọn token có xác suất cao nhất\n",
    "            predicted_id = logits.argmax(dim=1).item()  # scalar\n",
    "            output_ids.append(predicted_id)\n",
    "            \n",
    "            # Nếu gặp <eos>, dừng\n",
    "            if predicted_id == stoi_de[\"<eos>\"]:\n",
    "                break\n",
    "            \n",
    "            # input cho bước tiếp theo\n",
    "            input_token = torch.tensor([predicted_id], dtype=torch.long).to(device)\n",
    "    \n",
    "    # ---- 4. Detokenize: convert id → token → text ----\n",
    "    # Bỏ <sos> và <eos>\n",
    "    output_tokens = [itos_de.get(idx, \"<unk>\") for idx in output_ids[1:]]\n",
    "    if output_tokens and output_tokens[-1] == \"<eos>\":\n",
    "        output_tokens = output_tokens[:-1]\n",
    "    \n",
    "    translated_sentence = detokenize_de(output_tokens)\n",
    "    \n",
    "    return translated_sentence\n",
    "\n",
    "\n",
    "# ---- Test examples ----\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What is your name?\",\n",
    "    \"The weather is nice today.\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INFERENCE EXAMPLES (Greedy Decoding)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for en_sent in test_sentences:\n",
    "    de_sent = translate(en_sent, model, device, tokenizer_en, stoi_en, itos_de, stoi_de)\n",
    "    print(f\"EN: {en_sent}\")\n",
    "    print(f\"DE: {de_sent}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db6b62c",
   "metadata": {},
   "source": [
    "# 10. Đánh giá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33429955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 200 test sentences\n",
      "\n",
      "Translating test set...\n",
      "Translated 200 sentences\n",
      "\n",
      "======================================================================\n",
      "EVALUATION METRICS\n",
      "======================================================================\n",
      "\n",
      "BLEU Score (average): 0.0059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 644.2562\n",
      "Average Loss: 6.4681\n",
      "\n",
      "======================================================================\n",
      "DETAILED EXAMPLES & ERROR ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "--- TOP 5 BEST TRANSLATIONS (Highest BLEU) ---\n",
      "\n",
      "1. BLEU: 0.4111\n",
      "   EN:  A woman sits at a dark bar.\n",
      "   REF: Eine Frau sitzt an einer dunklen Bar.\n",
      "   PRED: eine frau sitzt an einer dunklen bar.\n",
      "\n",
      "2. BLEU: 0.3156\n",
      "   EN:  A man playing a keyboard and singing into a microphone.\n",
      "   REF: Eine Frau spielt Keyboard und singt in ein Mikrofon.\n",
      "   PRED: ein mann spielt keyboard und singt in ein mikrophon.\n",
      "\n",
      "3. BLEU: 0.2790\n",
      "   EN:  A man sleeping in a green room on a couch.\n",
      "   REF: Ein Mann schläft in einem grünen Raum auf einem Sofa.\n",
      "   PRED: ein mann schläft in einem grünen grünen auf einem grünen sofa.\n",
      "\n",
      "4. BLEU: 0.1750\n",
      "   EN:  A balding man wearing a red life jacket is sitting in a small boat.\n",
      "   REF: Ein Mann mit beginnender Glatze, der eine rote Rettungsweste trägt, sitzt in einem kleinen Boot.\n",
      "   PRED: ein mann mit einer roten schwimmweste sitzt in einem kleinen boot auf einem boot.\n",
      "\n",
      "5. BLEU: 0.0000\n",
      "   EN:  They are posing for a picture.\n",
      "   REF: Sie posieren für ein Bild.\n",
      "   PRED: sie posieren für ein foto.\n",
      "\n",
      "\n",
      "--- TOP 5 WORST TRANSLATIONS (Lowest BLEU) ---\n",
      "\n",
      "1. BLEU: 0.0000\n",
      "   EN:  A cute baby is smiling at another child.\n",
      "   REF: Ein süßes Baby lächelt einem anderen Kind zu.\n",
      "   PRED: ein baby baby lächelt, während ein kind kind.\n",
      "   ERROR ANALYSIS:\n",
      "     - Missing words: anderen, Kind, Baby, lächelt, süßes\n",
      "     - Extra words: lächelt,, ein, baby, kind, kind.\n",
      "\n",
      "2. BLEU: 0.0000\n",
      "   EN:  A tractor is moving dirt to help build up a retaining wall.\n",
      "   REF: Ein Traktor bewegt Erde für den Bau einer Stützmauer.\n",
      "   PRED: ein rennauto zerbricht, um eine <unk>, um eine <unk>.\n",
      "   ERROR ANALYSIS:\n",
      "     - Missing words: den, bewegt, Traktor, für, Ein\n",
      "     - Extra words: ein, um, zerbricht,, <unk>., rennauto\n",
      "\n",
      "3. BLEU: 0.0000\n",
      "   EN:  Beach goers look at a heart left in the sky by a sky writer.\n",
      "   REF: Strandbesucher blicken auf ein Herz, das von einem Himmelsschreiber am Himmel hinterlassen wurde.\n",
      "   PRED: <unk> lehnt sich an den fenster eines <unk> <unk> <unk> <unk>.\n",
      "   ERROR ANALYSIS:\n",
      "     - Missing words: Himmelsschreiber, Herz,, von, blicken, ein\n",
      "     - Extra words: den, eines, an, lehnt, <unk>.\n",
      "\n",
      "4. BLEU: 0.0000\n",
      "   EN:  A man in a harness climbing a rock wall\n",
      "   REF: Ein Mann in einem Klettergurt klettert an einer Felswand\n",
      "   PRED: ein mann, der einen einen felswand hoch.\n",
      "   ERROR ANALYSIS:\n",
      "     - Missing words: an, Klettergurt, in, Ein, einer\n",
      "     - Extra words: ein, einen, der, mann,, hoch.\n",
      "\n",
      "5. BLEU: 0.0000\n",
      "   EN:  A child that is dressed as Spiderman is ringing the doorbell.\n",
      "   REF: Ein als Spiderman verkleidetes Kind klingelt an der Tür.\n",
      "   PRED: ein kind ist das wie es verkleidet ist.\n",
      "   ERROR ANALYSIS:\n",
      "     - Missing words: an, Kind, klingelt, als, Ein\n",
      "     - Extra words: ein, ist., verkleidet, kind, das\n",
      "\n",
      "\n",
      "======================================================================\n",
      "BLEU SCORE DISTRIBUTION\n",
      "======================================================================\n",
      "\n",
      "Min BLEU:    0.0000\n",
      "Max BLEU:    0.4111\n",
      "Mean BLEU:   0.0059\n",
      "Median BLEU: 0.0000\n",
      "Std BLEU:    0.0430\n",
      "\n",
      "BLEU Score Distribution by Range:\n",
      "  0.0-0.2:  197 ( 98.5%)\n",
      "  0.2-0.4:    2 (  1.0%)\n",
      "  0.4-0.6:    1 (  0.5%)\n",
      "  0.6-0.8:    0 (  0.0%)\n",
      "  0.8-1.0:    0 (  0.0%)\n",
      "\n",
      "======================================================================\n",
      "COMMON ERROR PATTERNS\n",
      "======================================================================\n",
      "\n",
      "Error Pattern Frequencies (out of 200 sentences):\n",
      "  length_mismatch:  134 ( 67.0%)\n",
      "  word_substitution:  200 (100.0%)\n",
      "  omission:    3 (  1.5%)\n",
      "  insertion:    3 (  1.5%)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Load test set (hoặc dùng tập val nếu không có test riêng)\n",
    "# test_en, test_de = load_parallel_corpus(\"test.en.gz\", \"test.de.gz\")\n",
    "# Tạm dùng val set để demo\n",
    "test_en, test_de = val_en[:200], val_de[:200]  # Lấy 200 câu từ val set\n",
    "\n",
    "print(f\"Evaluating on {len(test_en)} test sentences\")\n",
    "\n",
    "# ========== 1. Tính BLEU Score ==========\n",
    "\n",
    "def compute_bleu_score(references, hypotheses):\n",
    "    \"\"\"\n",
    "    Tính BLEU score trung bình trên corpus\n",
    "    \n",
    "    Args:\n",
    "        references: list of list of reference sentences (tokens)\n",
    "        hypotheses: list of hypothesis sentences (tokens)\n",
    "    \n",
    "    Returns:\n",
    "        bleu_score (0-1)\n",
    "    \"\"\"\n",
    "    total_bleu = 0.0\n",
    "    n = len(hypotheses)\n",
    "    \n",
    "    bleu_scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        # sentence_bleu expects: reference (list of list), hypothesis (list)\n",
    "        ref_tokens = ref.split()\n",
    "        hyp_tokens = hyp.split()\n",
    "        \n",
    "        # weights for 1-gram, 2-gram, 3-gram, 4-gram\n",
    "        weights = (0.25, 0.25, 0.25, 0.25)\n",
    "        bleu = sentence_bleu([ref_tokens], hyp_tokens, weights=weights)\n",
    "        bleu_scores.append(bleu)\n",
    "        total_bleu += bleu\n",
    "    \n",
    "    avg_bleu = total_bleu / n\n",
    "    return avg_bleu, bleu_scores\n",
    "\n",
    "\n",
    "# ========== 2. Tính Perplexity ==========\n",
    "\n",
    "def compute_perplexity(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Tính Perplexity trên test set\n",
    "    Perplexity = exp(loss)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, src_lengths, trg, trg_lengths in test_loader:\n",
    "            src = src.to(device)\n",
    "            src_lengths = src_lengths.to(device)\n",
    "            trg = trg.to(device)\n",
    "            \n",
    "            outputs = model(src, src_lengths, trg)\n",
    "            vocab_size = outputs.size(-1)\n",
    "            \n",
    "            pred = outputs[:, 1:, :].contiguous().view(-1, vocab_size)\n",
    "            target = trg[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(pred, target)\n",
    "            total_loss += loss.item() * target.size(0)\n",
    "            n_tokens += (target != PAD_IDX_DE).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / n_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    return perplexity, avg_loss\n",
    "\n",
    "\n",
    "# ========== 3. Tạo Test DataLoader ==========\n",
    "\n",
    "test_dataset = make_dataset(test_en, test_de, tokenizer_en, tokenizer_de, stoi_en, stoi_de)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "# ========== 4. Dịch toàn bộ test set ==========\n",
    "\n",
    "print(\"\\nTranslating test set...\")\n",
    "predictions = []\n",
    "for en_sent in test_en:\n",
    "    de_pred = translate(en_sent, model, device, tokenizer_en, stoi_en, itos_de, stoi_de)\n",
    "    predictions.append(de_pred)\n",
    "\n",
    "print(f\"Translated {len(predictions)} sentences\")\n",
    "\n",
    "\n",
    "# ========== 5. Tính BLEU & Perplexity ==========\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# BLEU Score\n",
    "avg_bleu, bleu_scores = compute_bleu_score(test_de, predictions)\n",
    "print(f\"\\nBLEU Score (average): {avg_bleu:.4f}\")\n",
    "\n",
    "# Perplexity\n",
    "perplexity, avg_loss = compute_perplexity(model, test_loader, criterion, device)\n",
    "print(f\"Perplexity: {perplexity:.4f}\")\n",
    "print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "# ========== 6. Error Analysis: 5 ví dụ đúng + sai ==========\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETAILED EXAMPLES & ERROR ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sắp xếp theo BLEU score để lấy ví dụ tốt nhất và xấu nhất\n",
    "indices = np.argsort(bleu_scores)\n",
    "\n",
    "# 5 ví dụ tốt nhất (highest BLEU)\n",
    "print(\"\\n--- TOP 5 BEST TRANSLATIONS (Highest BLEU) ---\\n\")\n",
    "best_indices = indices[-5:][::-1]\n",
    "for rank, idx in enumerate(best_indices, 1):\n",
    "    en = test_en[idx]\n",
    "    de_ref = test_de[idx]\n",
    "    de_pred = predictions[idx]\n",
    "    bleu = bleu_scores[idx]\n",
    "    \n",
    "    print(f\"{rank}. BLEU: {bleu:.4f}\")\n",
    "    print(f\"   EN:  {en}\")\n",
    "    print(f\"   REF: {de_ref}\")\n",
    "    print(f\"   PRED: {de_pred}\")\n",
    "    print()\n",
    "\n",
    "# 5 ví dụ tệ nhất (lowest BLEU)\n",
    "print(\"\\n--- TOP 5 WORST TRANSLATIONS (Lowest BLEU) ---\\n\")\n",
    "worst_indices = indices[:5]\n",
    "for rank, idx in enumerate(worst_indices, 1):\n",
    "    en = test_en[idx]\n",
    "    de_ref = test_de[idx]\n",
    "    de_pred = predictions[idx]\n",
    "    bleu = bleu_scores[idx]\n",
    "    \n",
    "    print(f\"{rank}. BLEU: {bleu:.4f}\")\n",
    "    print(f\"   EN:  {en}\")\n",
    "    print(f\"   REF: {de_ref}\")\n",
    "    print(f\"   PRED: {de_pred}\")\n",
    "    \n",
    "    # Phân tích lỗi\n",
    "    ref_tokens = set(de_ref.split())\n",
    "    pred_tokens = set(de_pred.split())\n",
    "    \n",
    "    missing = ref_tokens - pred_tokens\n",
    "    extra = pred_tokens - ref_tokens\n",
    "    \n",
    "    if missing or extra:\n",
    "        print(f\"   ERROR ANALYSIS:\")\n",
    "        if missing:\n",
    "            print(f\"     - Missing words: {', '.join(list(missing)[:5])}\")\n",
    "        if extra:\n",
    "            print(f\"     - Extra words: {', '.join(list(extra)[:5])}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# ========== 7. Thống kê BLEU Distribution ==========\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BLEU SCORE DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "bleu_array = np.array(bleu_scores)\n",
    "print(f\"\\nMin BLEU:    {bleu_array.min():.4f}\")\n",
    "print(f\"Max BLEU:    {bleu_array.max():.4f}\")\n",
    "print(f\"Mean BLEU:   {bleu_array.mean():.4f}\")\n",
    "print(f\"Median BLEU: {np.median(bleu_array):.4f}\")\n",
    "print(f\"Std BLEU:    {bleu_array.std():.4f}\")\n",
    "\n",
    "# Phân loại theo BLEU ranges\n",
    "bleu_ranges = {\n",
    "    \"0.0-0.2\": (bleu_array >= 0.0) & (bleu_array < 0.2),\n",
    "    \"0.2-0.4\": (bleu_array >= 0.2) & (bleu_array < 0.4),\n",
    "    \"0.4-0.6\": (bleu_array >= 0.4) & (bleu_array < 0.6),\n",
    "    \"0.6-0.8\": (bleu_array >= 0.6) & (bleu_array < 0.8),\n",
    "    \"0.8-1.0\": (bleu_array >= 0.8) & (bleu_array <= 1.0),\n",
    "}\n",
    "\n",
    "print(\"\\nBLEU Score Distribution by Range:\")\n",
    "for range_name, mask in bleu_ranges.items():\n",
    "    count = mask.sum()\n",
    "    pct = 100 * count / len(bleu_array)\n",
    "    print(f\"  {range_name}: {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "\n",
    "# ========== 8. Common Error Patterns ==========\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMMON ERROR PATTERNS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "error_patterns = {\n",
    "    \"length_mismatch\": 0,\n",
    "    \"word_substitution\": 0,\n",
    "    \"omission\": 0,\n",
    "    \"insertion\": 0,\n",
    "}\n",
    "\n",
    "for idx in range(len(test_de)):\n",
    "    ref_tokens = test_de[idx].split()\n",
    "    pred_tokens = predictions[idx].split()\n",
    "    \n",
    "    if len(pred_tokens) < len(ref_tokens) * 0.7:\n",
    "        error_patterns[\"omission\"] += 1\n",
    "    elif len(pred_tokens) > len(ref_tokens) * 1.3:\n",
    "        error_patterns[\"insertion\"] += 1\n",
    "    elif len(pred_tokens) != len(ref_tokens):\n",
    "        error_patterns[\"length_mismatch\"] += 1\n",
    "    \n",
    "    if ref_tokens != pred_tokens:\n",
    "        # Check for word substitutions\n",
    "        matching = sum(1 for r, p in zip(ref_tokens, pred_tokens) if r == p)\n",
    "        if matching < len(ref_tokens):\n",
    "            error_patterns[\"word_substitution\"] += 1\n",
    "\n",
    "print(\"\\nError Pattern Frequencies (out of {} sentences):\".format(len(test_de)))\n",
    "for pattern, count in error_patterns.items():\n",
    "    pct = 100 * count / len(test_de)\n",
    "    print(f\"  {pattern}: {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6101a1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Evaluation charts saved as 'evaluation_metrics.png'\n",
      "\n",
      "======================================================================\n",
      "END EVALUATION & VISUALIZATION\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_16416\\2843494803.py:145: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# ========== 9. BIỂU ĐỒ ĐÁNH GIÁ (Evaluation Charts) ==========\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend\n",
    "import seaborn as sns\n",
    "\n",
    "# Cài đặt style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Tạo figure với 3 subplots\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# ===== Biểu đồ 1: Phân phối BLEU Scores (Histogram) =====\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "bleu_array = np.array(bleu_scores)\n",
    "ax1.hist(bleu_array, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(bleu_array.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {bleu_array.mean():.4f}')\n",
    "ax1.axvline(np.median(bleu_array), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(bleu_array):.4f}')\n",
    "ax1.set_xlabel('BLEU Score', fontsize=11)\n",
    "ax1.set_ylabel('Frequency', fontsize=11)\n",
    "ax1.set_title('Distribution of BLEU Scores', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ===== Biểu đồ 2: BLEU Score Ranges (Bar Chart) =====\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "bleu_ranges = {\n",
    "    \"0.0-0.2\": (bleu_array >= 0.0) & (bleu_array < 0.2),\n",
    "    \"0.2-0.4\": (bleu_array >= 0.2) & (bleu_array < 0.4),\n",
    "    \"0.4-0.6\": (bleu_array >= 0.4) & (bleu_array < 0.6),\n",
    "    \"0.6-0.8\": (bleu_array >= 0.6) & (bleu_array < 0.8),\n",
    "    \"0.8-1.0\": (bleu_array >= 0.8) & (bleu_array <= 1.0),\n",
    "}\n",
    "range_names = list(bleu_ranges.keys())\n",
    "range_counts = [bleu_ranges[r].sum() for r in range_names]\n",
    "colors = ['#FF6B6B', '#FFA06B', '#FFD93D', '#6BCB77', '#4D96FF']\n",
    "bars = ax2.bar(range_names, range_counts, color=colors, edgecolor='black', alpha=0.8)\n",
    "ax2.set_xlabel('BLEU Score Range', fontsize=11)\n",
    "ax2.set_ylabel('Number of Sentences', fontsize=11)\n",
    "ax2.set_title('BLEU Score Distribution by Range', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# ===== Biểu đồ 3: Error Patterns (Bar Chart) =====\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "error_pattern_names = list(error_patterns.keys())\n",
    "error_pattern_counts = [error_patterns[p] for p in error_pattern_names]\n",
    "colors_errors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "bars3 = ax3.bar(error_pattern_names, error_pattern_counts, color=colors_errors, edgecolor='black', alpha=0.8)\n",
    "ax3.set_xlabel('Error Type', fontsize=11)\n",
    "ax3.set_ylabel('Frequency', fontsize=11)\n",
    "ax3.set_title('Common Error Pattern Distribution', fontsize=12, fontweight='bold')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "# Add value labels\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# ===== Biểu đồ 4: Sentence Length vs BLEU Score (Scatter Plot) =====\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "en_lengths = [len(s.split()) for s in test_en]\n",
    "scatter = ax4.scatter(en_lengths, bleu_scores, alpha=0.6, c=bleu_scores, \n",
    "                      cmap='RdYlGn', s=50, edgecolors='black', linewidth=0.5)\n",
    "ax4.set_xlabel('Input Sentence Length (tokens)', fontsize=11)\n",
    "ax4.set_ylabel('BLEU Score', fontsize=11)\n",
    "ax4.set_title('BLEU Score vs Sentence Length', fontsize=12, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "cbar = plt.colorbar(scatter, ax=ax4)\n",
    "cbar.set_label('BLEU Score', fontsize=10)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(en_lengths, bleu_scores, 2)\n",
    "p = np.poly1d(z)\n",
    "x_trend = np.linspace(min(en_lengths), max(en_lengths), 100)\n",
    "ax4.plot(x_trend, p(x_trend), \"r--\", linewidth=2, alpha=0.8, label='Trend')\n",
    "ax4.legend()\n",
    "\n",
    "# ===== Biểu đồ 5: Cumulative BLEU Distribution =====\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "sorted_bleu = np.sort(bleu_scores)\n",
    "cumulative = np.arange(1, len(sorted_bleu) + 1) / len(sorted_bleu) * 100\n",
    "ax5.plot(sorted_bleu, cumulative, linewidth=2.5, color='darkblue', marker='o', markersize=4, alpha=0.7)\n",
    "ax5.axhline(y=50, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='50th percentile')\n",
    "ax5.axhline(y=75, color='orange', linestyle='--', linewidth=1.5, alpha=0.7, label='75th percentile')\n",
    "ax5.axhline(y=90, color='green', linestyle='--', linewidth=1.5, alpha=0.7, label='90th percentile')\n",
    "ax5.set_xlabel('BLEU Score', fontsize=11)\n",
    "ax5.set_ylabel('Cumulative Percentage (%)', fontsize=11)\n",
    "ax5.set_title('Cumulative BLEU Score Distribution', fontsize=12, fontweight='bold')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# ===== Biểu đồ 6: Performance Metrics Summary (Text Summary) =====\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "ax6.axis('off')\n",
    "\n",
    "# Tính các metrics\n",
    "correct_translations = sum(1 for ref, pred in zip(test_de, predictions) if ref == pred)\n",
    "accuracy = 100 * correct_translations / len(test_de)\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "EVALUATION SUMMARY\n",
    "\n",
    "Total Sentences: {len(test_de)}\n",
    "Correct Translations: {correct_translations} ({accuracy:.1f}%)\n",
    "\n",
    "BLEU Score Statistics:\n",
    "• Mean: {bleu_array.mean():.4f}\n",
    "• Median: {np.median(bleu_array):.4f}\n",
    "• Min: {bleu_array.min():.4f}\n",
    "• Max: {bleu_array.max():.4f}\n",
    "• Std Dev: {bleu_array.std():.4f}\n",
    "\n",
    "Perplexity: {perplexity:.4f}\n",
    "Loss: {avg_loss:.4f}\n",
    "\n",
    "Sentence Length Statistics:\n",
    "• Mean EN Length: {np.mean(en_lengths):.1f}\n",
    "• Mean DE Length: {np.mean([len(s.split()) for s in test_de]):.1f}\n",
    "• Max EN Length: {max(en_lengths)}\n",
    "• Max DE Length: {max([len(s.split()) for s in test_de])}\n",
    "\n",
    "Error Analysis:\n",
    "• Omission Rate: {error_patterns['omission']/len(test_de)*100:.1f}%\n",
    "• Insertion Rate: {error_patterns['insertion']/len(test_de)*100:.1f}%\n",
    "• Word Substitution: {error_patterns['word_substitution']/len(test_de)*100:.1f}%\n",
    "\"\"\"\n",
    "\n",
    "ax6.text(0.1, 0.95, summary_text, transform=ax6.transAxes, \n",
    "        fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('evaluation_metrics.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n✓ Evaluation charts saved as 'evaluation_metrics.png'\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"END EVALUATION & VISUALIZATION\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77668fe9",
   "metadata": {},
   "source": [
    "# 11. Xử lý các phần khó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "527b1808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TROUBLESHOOTING & DEBUGGING GUIDE\n",
      "================================================================================\n",
      "\n",
      "[1] CHECKING TENSOR SHAPES\n",
      "--------------------------------------------------------------------------------\n",
      "Sample batch shapes:\n",
      "  src shape:          torch.Size([64, 25]) (batch, seq_len)\n",
      "  src_lengths shape:  torch.Size([64])\n",
      "  trg shape:          torch.Size([64, 27])\n",
      "  trg_lengths shape:  torch.Size([64])\n",
      "\n",
      "  model output shape: torch.Size([64, 27, 10000]) (batch, seq_len, vocab_size)\n",
      "  Expected: (64, 27, 10000)\n",
      "\n",
      "  pred shape (after reshape): torch.Size([1664, 10000])\n",
      "  target shape (after reshape): torch.Size([1664])\n",
      "  loss: 1.4158\n",
      "\n",
      "\n",
      "[2] CHECKING DATA NORMALIZATION\n",
      "--------------------------------------------------------------------------------\n",
      "English sentence lengths:\n",
      "  Min: 3, Max: 37, Mean: 11.9\n",
      "  Median: 11.0, Std: 3.8\n",
      "\n",
      "German sentence lengths:\n",
      "  Min: 1, Max: 39, Mean: 11.1\n",
      "  Median: 11.0, Std: 3.8\n",
      "\n",
      "Sentences longer than 50 tokens:\n",
      "  EN: 0 (0.0%)\n",
      "  DE: 0 (0.0%)\n",
      "\n",
      "\n",
      "[3] CHECKING LEARNING RATE & GRADIENTS\n",
      "--------------------------------------------------------------------------------\n",
      "Gradient Norm: 0.5599\n",
      "Learning Rate: 0.001000\n",
      "✓ Gradient norm looks reasonable\n",
      "\n",
      "\n",
      "[4] TEACHER FORCING & EXPOSURE BIAS\n",
      "--------------------------------------------------------------------------------\n",
      "Current teacher_forcing_ratio: 0.5\n",
      "\n",
      "Recommendations:\n",
      "  - Start with 0.5 (50% ground truth, 50% predictions)\n",
      "  - Use scheduled sampling: gradually decrease ratio during training\n",
      "  - Formula: tf_ratio = initial * exp(-decay * epoch)\n",
      "\n",
      "Implementation example:\n",
      "\n",
      "# Scheduled teacher forcing\n",
      "def get_tf_ratio(epoch, initial_tf=0.5, decay=0.05):\n",
      "    return initial_tf * math.exp(-decay * epoch)\n",
      "\n",
      "# In training loop:\n",
      "model.teacher_forcing_ratio = get_tf_ratio(epoch)\n",
      "\n",
      "\n",
      "\n",
      "[5] OVERFITTING DETECTION\n",
      "--------------------------------------------------------------------------------\n",
      "Training Loss (first vs last): 4.7478 → 1.6408\n",
      "Validation Loss (first vs last): 3.6906 → 2.3131\n",
      "Train-Val Gap: 0.6723\n",
      "\n",
      "⚠️  WARNING: Significant overfitting detected!\n",
      "\n",
      "Solutions:\n",
      "  1. Increase dropout (currently 0.3)\n",
      "  2. Add L2 regularization (weight decay)\n",
      "  3. Use early stopping (already enabled)\n",
      "  4. Filter long sentences (max 50 tokens)\n",
      "  5. Increase batch size\n",
      "\n",
      "\n",
      "[6] DIAGNOSING 'LOSS NOT DECREASING' ISSUES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Common causes and solutions:\n",
      "\n",
      "1. LEARNING RATE TOO HIGH\n",
      "   - Symptom: Loss oscillates or increases\n",
      "   - Solution: Reduce LR (e.g., 0.001 → 0.0005)\n",
      "\n",
      "2. LEARNING RATE TOO LOW\n",
      "   - Symptom: Loss decreases very slowly\n",
      "   - Solution: Increase LR (e.g., 0.0001 → 0.001)\n",
      "\n",
      "3. GRADIENT VANISHING/EXPLODING\n",
      "   - Symptom: Loss becomes NaN or Inf\n",
      "   - Solution: Check gradient norm, increase CLIP value, use gradient clipping\n",
      "\n",
      "4. BAD DATA\n",
      "   - Symptom: Loss plateaus at high value\n",
      "   - Solution: Check data quality, verify tokenization, ensure padding is correct\n",
      "\n",
      "5. MODEL TOO SMALL\n",
      "   - Symptom: Slow improvement on training set\n",
      "   - Solution: Increase embed_dim, hidden_size, or num_layers\n",
      "\n",
      "6. BATCH SIZE ISSUES\n",
      "   - Too small: Noisy gradients, slow training\n",
      "   - Too large: Memory issues, poor generalization\n",
      "   - Try: 32, 64, 128\n",
      "\n",
      "\n",
      "\n",
      "[7] MEMORY & PERFORMANCE OPTIMIZATION\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Memory-saving strategies:\n",
      "\n",
      "1. FILTER LONG SENTENCES\n",
      "   - Limit to max_len=50 tokens\n",
      "   - Code example:\n",
      "\n",
      "   def filter_by_length(en_sents, de_sents, max_len=50):\n",
      "       data = [(en, de) for en, de in zip(en_sents, de_sents)\n",
      "               if len(en.split()) <= max_len and len(de.split()) <= max_len]\n",
      "       en_filtered, de_filtered = zip(*data)\n",
      "       return list(en_filtered), list(de_filtered)\n",
      "\n",
      "   train_en, train_de = filter_by_length(train_en, train_de, max_len=50)\n",
      "\n",
      "2. REDUCE VOCAB SIZE\n",
      "   - Currently: 10,000 words\n",
      "   - Try: 5,000 or 8,000\n",
      "   - Trade-off: Less <unk> tokens vs smaller model\n",
      "\n",
      "3. REDUCE EMBEDDING/HIDDEN DIMENSION\n",
      "   - Current: embed_dim=512, hidden_size=512\n",
      "   - Try: 256 or 384\n",
      "   - Still gets decent results with lower memory\n",
      "\n",
      "4. USE GRADIENT ACCUMULATION (if needed)\n",
      "   - Simulate larger batch size with smaller batches\n",
      "\n",
      "5. MIXED PRECISION (if using CUDA)\n",
      "   - Use torch.cuda.amp for faster computation\n",
      "\n",
      "\n",
      "\n",
      "[8] TRAINING MONITORING CHECKLIST\n",
      "--------------------------------------------------------------------------------\n",
      "  ☐ Tensor shapes                  - ✓ Verify in [1]\n",
      "  ☐ Data stats                     - ✓ Check in [2]\n",
      "  ☐ Gradient flow                  - ✓ Monitor in [3]\n",
      "  ☐ Teacher forcing                - ✓ Review in [4]\n",
      "  ☐ Overfitting                    - ✓ Assess in [5]\n",
      "  ☐ Learning rate                  - Adjust based on loss curve\n",
      "  ☐ Loss trend                     - Should decrease monotonically (with fluctuations)\n",
      "  ☐ Validation loss                - Should decrease, gap with train loss < 0.5\n",
      "  ☐ Checkpoints                    - Save best model (already doing)\n",
      "  ☐ Early stopping                 - Patience=3 (already enabled)\n",
      "\n",
      "\n",
      "[9] QUICK DEBUG: Run this if loss gets stuck\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "# Step 1: Check a single batch\n",
      "src, src_lengths, trg, trg_lengths = next(iter(train_loader))\n",
      "print(\"Input shapes OK:\", src.shape, trg.shape)\n",
      "\n",
      "# Step 2: Forward pass\n",
      "model.eval()\n",
      "with torch.no_grad():\n",
      "    out = model(src.to(device), src_lengths.to(device), trg.to(device))\n",
      "    print(\"Output shape OK:\", out.shape)\n",
      "\n",
      "# Step 3: Compute loss manually\n",
      "pred = out[:, 1:, :].contiguous().view(-1, len(vocab_de))\n",
      "target = trg[:, 1:].contiguous().view(-1)\n",
      "loss = criterion(pred, target)\n",
      "print(\"Loss OK:\", loss.item())\n",
      "\n",
      "# Step 4: Check for NaN/Inf\n",
      "print(\"Contains NaN:\", torch.isnan(out).any().item())\n",
      "print(\"Contains Inf:\", torch.isinf(out).any().item())\n",
      "\n",
      "\n",
      "================================================================================\n",
      "END TROUBLESHOOTING GUIDE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TROUBLESHOOTING & DEBUGGING GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== 1. Kiểm tra Shape của Tensors ==========\n",
    "\n",
    "print(\"\\n[1] CHECKING TENSOR SHAPES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def check_tensor_shapes():\n",
    "    \"\"\"Kiểm tra shape của các tensor trong training\"\"\"\n",
    "    print(\"Sample batch shapes:\")\n",
    "    \n",
    "    # Lấy một batch để kiểm tra\n",
    "    for src, src_lengths, trg, trg_lengths in train_loader:\n",
    "        print(f\"  src shape:          {src.shape} (batch, seq_len)\")\n",
    "        print(f\"  src_lengths shape:  {src_lengths.shape}\")\n",
    "        print(f\"  trg shape:          {trg.shape}\")\n",
    "        print(f\"  trg_lengths shape:  {trg_lengths.shape}\")\n",
    "        \n",
    "        src = src.to(device)\n",
    "        src_lengths = src_lengths.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        # Forward pass (training mode)\n",
    "        model.train()\n",
    "        outputs = model(src, src_lengths, trg)\n",
    "        \n",
    "        print(f\"\\n  model output shape: {outputs.shape} (batch, seq_len, vocab_size)\")\n",
    "        print(f\"  Expected: ({src.size(0)}, {trg.size(1)}, {len(vocab_de)})\")\n",
    "        \n",
    "        # Kiểm tra loss\n",
    "        vocab_size = outputs.size(-1)\n",
    "        pred = outputs[:, 1:, :].contiguous().view(-1, vocab_size)\n",
    "        target = trg[:, 1:].contiguous().view(-1)\n",
    "        \n",
    "        print(f\"\\n  pred shape (after reshape): {pred.shape}\")\n",
    "        print(f\"  target shape (after reshape): {target.shape}\")\n",
    "        \n",
    "        loss = criterion(pred, target)\n",
    "        print(f\"  loss: {loss.item():.4f}\")\n",
    "        \n",
    "        break\n",
    "\n",
    "check_tensor_shapes()\n",
    "\n",
    "\n",
    "# ========== 2. Kiểm tra Data Normalization ==========\n",
    "\n",
    "print(\"\\n\\n[2] CHECKING DATA NORMALIZATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def check_data_stats():\n",
    "    \"\"\"Kiểm tra thống kê dữ liệu: độ dài câu, phân bố từ\"\"\"\n",
    "    \n",
    "    # Độ dài câu\n",
    "    en_lengths = [len(s.split()) for s in train_en]\n",
    "    de_lengths = [len(s.split()) for s in train_de]\n",
    "    \n",
    "    print(\"English sentence lengths:\")\n",
    "    print(f\"  Min: {min(en_lengths)}, Max: {max(en_lengths)}, Mean: {np.mean(en_lengths):.1f}\")\n",
    "    print(f\"  Median: {np.median(en_lengths):.1f}, Std: {np.std(en_lengths):.1f}\")\n",
    "    \n",
    "    print(\"\\nGerman sentence lengths:\")\n",
    "    print(f\"  Min: {min(de_lengths)}, Max: {max(de_lengths)}, Mean: {np.mean(de_lengths):.1f}\")\n",
    "    print(f\"  Median: {np.median(de_lengths):.1f}, Std: {np.std(de_lengths):.1f}\")\n",
    "    \n",
    "    # Cảnh báo nếu có câu quá dài\n",
    "    max_len_threshold = 50\n",
    "    en_too_long = sum(1 for l in en_lengths if l > max_len_threshold)\n",
    "    de_too_long = sum(1 for l in de_lengths if l > max_len_threshold)\n",
    "    \n",
    "    print(f\"\\nSentences longer than {max_len_threshold} tokens:\")\n",
    "    print(f\"  EN: {en_too_long} ({100*en_too_long/len(en_lengths):.1f}%)\")\n",
    "    print(f\"  DE: {de_too_long} ({100*de_too_long/len(de_lengths):.1f}%)\")\n",
    "    \n",
    "    if en_too_long > 0 or de_too_long > 0:\n",
    "        print(\"\\n  ⚠️ TIP: Consider filtering sentences > 50 tokens to reduce memory usage\")\n",
    "        print(\"         and improve training stability\")\n",
    "\n",
    "check_data_stats()\n",
    "\n",
    "\n",
    "# ========== 3. Learning Rate & Gradient Check ==========\n",
    "\n",
    "print(\"\\n\\n[3] CHECKING LEARNING RATE & GRADIENTS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def check_gradients():\n",
    "    \"\"\"Kiểm tra gradient flow\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Lấy một batch\n",
    "    for src, src_lengths, trg, trg_lengths in train_loader:\n",
    "        src = src.to(device)\n",
    "        src_lengths = src_lengths.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(src, src_lengths, trg)\n",
    "        vocab_size = outputs.size(-1)\n",
    "        \n",
    "        pred = outputs[:, 1:, :].contiguous().view(-1, vocab_size)\n",
    "        target = trg[:, 1:].contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Kiểm tra gradient norm\n",
    "        total_norm = 0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "        \n",
    "        print(f\"Gradient Norm: {total_norm:.4f}\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        if total_norm > 100:\n",
    "            print(\"⚠️  WARNING: Large gradient norm detected!\")\n",
    "            print(\"   - Consider increasing CLIP value or reducing learning rate\")\n",
    "        elif total_norm < 0.0001:\n",
    "            print(\"⚠️  WARNING: Very small gradient norm!\")\n",
    "            print(\"   - Check if loss is saturating or learning rate is too small\")\n",
    "        else:\n",
    "            print(\"✓ Gradient norm looks reasonable\")\n",
    "        \n",
    "        break\n",
    "\n",
    "check_gradients()\n",
    "\n",
    "\n",
    "# ========== 4. Teacher Forcing Analysis ==========\n",
    "\n",
    "print(\"\\n\\n[4] TEACHER FORCING & EXPOSURE BIAS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"Current teacher_forcing_ratio: {model.teacher_forcing_ratio}\")\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"  - Start with 0.5 (50% ground truth, 50% predictions)\")\n",
    "print(\"  - Use scheduled sampling: gradually decrease ratio during training\")\n",
    "print(\"  - Formula: tf_ratio = initial * exp(-decay * epoch)\")\n",
    "print(\"\\nImplementation example:\")\n",
    "print(\"\"\"\n",
    "# Scheduled teacher forcing\n",
    "def get_tf_ratio(epoch, initial_tf=0.5, decay=0.05):\n",
    "    return initial_tf * math.exp(-decay * epoch)\n",
    "\n",
    "# In training loop:\n",
    "model.teacher_forcing_ratio = get_tf_ratio(epoch)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ========== 5. Overfitting Check ==========\n",
    "\n",
    "print(\"\\n\\n[5] OVERFITTING DETECTION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if len(history[\"train_loss\"]) > 2 and len(history[\"val_loss\"]) > 2:\n",
    "    train_loss_trend = history[\"train_loss\"][-1] < history[\"train_loss\"][0]\n",
    "    val_loss_trend = history[\"val_loss\"][-1] > history[\"val_loss\"][0]\n",
    "    \n",
    "    gap = history[\"val_loss\"][-1] - history[\"train_loss\"][-1]\n",
    "    \n",
    "    print(f\"Training Loss (first vs last): {history['train_loss'][0]:.4f} → {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Validation Loss (first vs last): {history['val_loss'][0]:.4f} → {history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Train-Val Gap: {gap:.4f}\")\n",
    "    \n",
    "    if gap > 0.5:\n",
    "        print(\"\\n⚠️  WARNING: Significant overfitting detected!\")\n",
    "        print(\"\\nSolutions:\")\n",
    "        print(\"  1. Increase dropout (currently 0.3)\")\n",
    "        print(\"  2. Add L2 regularization (weight decay)\")\n",
    "        print(\"  3. Use early stopping (already enabled)\")\n",
    "        print(\"  4. Filter long sentences (max 50 tokens)\")\n",
    "        print(\"  5. Increase batch size\")\n",
    "    else:\n",
    "        print(\"\\n✓ Overfitting levels look reasonable\")\n",
    "else:\n",
    "    print(\"Not enough epochs completed yet to assess overfitting\")\n",
    "\n",
    "\n",
    "# ========== 6. Loss Not Decreasing - Diagnostic ==========\n",
    "\n",
    "print(\"\\n\\n[6] DIAGNOSING 'LOSS NOT DECREASING' ISSUES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "Common causes and solutions:\n",
    "\n",
    "1. LEARNING RATE TOO HIGH\n",
    "   - Symptom: Loss oscillates or increases\n",
    "   - Solution: Reduce LR (e.g., 0.001 → 0.0005)\n",
    "   \n",
    "2. LEARNING RATE TOO LOW\n",
    "   - Symptom: Loss decreases very slowly\n",
    "   - Solution: Increase LR (e.g., 0.0001 → 0.001)\n",
    "   \n",
    "3. GRADIENT VANISHING/EXPLODING\n",
    "   - Symptom: Loss becomes NaN or Inf\n",
    "   - Solution: Check gradient norm, increase CLIP value, use gradient clipping\n",
    "   \n",
    "4. BAD DATA\n",
    "   - Symptom: Loss plateaus at high value\n",
    "   - Solution: Check data quality, verify tokenization, ensure padding is correct\n",
    "   \n",
    "5. MODEL TOO SMALL\n",
    "   - Symptom: Slow improvement on training set\n",
    "   - Solution: Increase embed_dim, hidden_size, or num_layers\n",
    "   \n",
    "6. BATCH SIZE ISSUES\n",
    "   - Too small: Noisy gradients, slow training\n",
    "   - Too large: Memory issues, poor generalization\n",
    "   - Try: 32, 64, 128\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ========== 7. Memory & Performance Tips ==========\n",
    "\n",
    "print(\"\\n\\n[7] MEMORY & PERFORMANCE OPTIMIZATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "Memory-saving strategies:\n",
    "\n",
    "1. FILTER LONG SENTENCES\n",
    "   - Limit to max_len=50 tokens\n",
    "   - Code example:\n",
    "   \n",
    "   def filter_by_length(en_sents, de_sents, max_len=50):\n",
    "       data = [(en, de) for en, de in zip(en_sents, de_sents)\n",
    "               if len(en.split()) <= max_len and len(de.split()) <= max_len]\n",
    "       en_filtered, de_filtered = zip(*data)\n",
    "       return list(en_filtered), list(de_filtered)\n",
    "   \n",
    "   train_en, train_de = filter_by_length(train_en, train_de, max_len=50)\n",
    "\n",
    "2. REDUCE VOCAB SIZE\n",
    "   - Currently: 10,000 words\n",
    "   - Try: 5,000 or 8,000\n",
    "   - Trade-off: Less <unk> tokens vs smaller model\n",
    "\n",
    "3. REDUCE EMBEDDING/HIDDEN DIMENSION\n",
    "   - Current: embed_dim=512, hidden_size=512\n",
    "   - Try: 256 or 384\n",
    "   - Still gets decent results with lower memory\n",
    "\n",
    "4. USE GRADIENT ACCUMULATION (if needed)\n",
    "   - Simulate larger batch size with smaller batches\n",
    "   \n",
    "5. MIXED PRECISION (if using CUDA)\n",
    "   - Use torch.cuda.amp for faster computation\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ========== 8. Monitoring Checklist ==========\n",
    "\n",
    "print(\"\\n\\n[8] TRAINING MONITORING CHECKLIST\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "checklist = {\n",
    "    \"Tensor shapes\": \"✓ Verify in [1]\",\n",
    "    \"Data stats\": \"✓ Check in [2]\",\n",
    "    \"Gradient flow\": \"✓ Monitor in [3]\",\n",
    "    \"Teacher forcing\": \"✓ Review in [4]\",\n",
    "    \"Overfitting\": \"✓ Assess in [5]\",\n",
    "    \"Learning rate\": \"Adjust based on loss curve\",\n",
    "    \"Loss trend\": \"Should decrease monotonically (with fluctuations)\",\n",
    "    \"Validation loss\": \"Should decrease, gap with train loss < 0.5\",\n",
    "    \"Checkpoints\": \"Save best model (already doing)\",\n",
    "    \"Early stopping\": \"Patience=3 (already enabled)\",\n",
    "}\n",
    "\n",
    "for item, status in checklist.items():\n",
    "    print(f\"  ☐ {item:30s} - {status}\")\n",
    "\n",
    "\n",
    "# ========== 9. Quick Debugging Code ==========\n",
    "\n",
    "print(\"\\n\\n[9] QUICK DEBUG: Run this if loss gets stuck\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "debug_code = \"\"\"\n",
    "# Step 1: Check a single batch\n",
    "src, src_lengths, trg, trg_lengths = next(iter(train_loader))\n",
    "print(\"Input shapes OK:\", src.shape, trg.shape)\n",
    "\n",
    "# Step 2: Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(src.to(device), src_lengths.to(device), trg.to(device))\n",
    "    print(\"Output shape OK:\", out.shape)\n",
    "\n",
    "# Step 3: Compute loss manually\n",
    "pred = out[:, 1:, :].contiguous().view(-1, len(vocab_de))\n",
    "target = trg[:, 1:].contiguous().view(-1)\n",
    "loss = criterion(pred, target)\n",
    "print(\"Loss OK:\", loss.item())\n",
    "\n",
    "# Step 4: Check for NaN/Inf\n",
    "print(\"Contains NaN:\", torch.isnan(out).any().item())\n",
    "print(\"Contains Inf:\", torch.isinf(out).any().item())\n",
    "\"\"\"\n",
    "\n",
    "print(debug_code)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"END TROUBLESHOOTING GUIDE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dba9d1",
   "metadata": {},
   "source": [
    "# 12. Phân tích lỗi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7dcb8bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "12. ERROR ANALYSIS & IMPROVEMENT PROPOSALS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PART 1: COMMON ERRORS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "[1] OUT-OF-VOCABULARY (OOV) ERROR ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "English OOV Statistics:\n",
      "  Total OOV tokens: 23\n",
      "  Avg OOV per sentence: 0.12\n",
      "  Max OOV in a sentence: 2\n",
      "  Sentences with OOV: 20\n",
      "\n",
      "German OOV Statistics:\n",
      "  Total OOV tokens: 99\n",
      "  Avg OOV per sentence: 0.49\n",
      "  Max OOV in a sentence: 7\n",
      "  Sentences with OOV: 69\n",
      "\n",
      "OOV Impact on Errors:\n",
      "  Errors with OOV: 73 / 200 (36.5%)\n",
      "\n",
      "Examples of OOV-related errors:\n",
      "\n",
      "  Example 1:\n",
      "    EN (OOV: 0): A group of men are loading cotton onto a truck\n",
      "    DE ref (OOV: 1): Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen\n",
      "    DE pred: eine gruppe männer männern, einen <unk> aus.\n",
      "\n",
      "  Example 2:\n",
      "    EN (OOV: 0): Two men setting up a blue ice fishing hut on an iced over lake\n",
      "    DE ref (OOV: 2): Zwei Männer bauen eine blaue Eisfischerhütte auf einem zugefrorenen See auf\n",
      "    DE pred: zwei männer ziehen eine pause auf einem <unk> eine pause über eine <unk>.\n",
      "\n",
      "  Example 3:\n",
      "    EN (OOV: 1): A lady in a red coat, holding a bluish hand bag likely of asian descent, jumping off the ground for a snapshot.\n",
      "    DE ref (OOV: 2): Eine Frau in einem rotem Mantel, die eine vermutlich aus Asien stammende Handtasche in einem blauen Farbton hält, springt für einen Schnappschuss in die Luft.\n",
      "    DE pred: eine dame in einem roten pullover hält einen <unk> aus und hält sich die hand aus dem <unk> <unk>.\n",
      "\n",
      "  Example 4:\n",
      "    EN (OOV: 0): A young boy wearing a Giants jersey swings a baseball bat at an incoming pitch.\n",
      "    DE ref (OOV: 1): Ein kleiner Junge mit einem Giants-Trikot schwingt einen Baseballschläger in Richtung eines ankommenden Balls.\n",
      "    DE pred: ein junge, der einen baseballschläger trägt, schwingt einen baseballschläger aus einem.\n",
      "\n",
      "  Example 5:\n",
      "    EN (OOV: 0): A young child is standing alone on some jagged rocks.\n",
      "    DE ref (OOV: 1): Ein kleines Kind steht allein auf einem zerklüfteten Felsen.\n",
      "    DE pred: ein kleines kind steht allein allein auf einer <unk>.\n",
      "\n",
      "\n",
      "[2] LONG SENTENCE ERROR ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "Error Rate by Sentence Length:\n",
      "Length Range         Total      Errors     Error %   \n",
      "--------------------------------------------------\n",
      "5-9                59         59         100.0%\n",
      "10-14               93         93         100.0%\n",
      "15-19               37         37         100.0%\n",
      "20-24               9          9          100.0%\n",
      "25-29               2          2          100.0%\n",
      "\n",
      "Sentence Length Statistics:\n",
      "  Min length: 6\n",
      "  Max length: 27\n",
      "  Mean: 12.0\n",
      "  Median: 11.0\n",
      "\n",
      "Examples of long sentences with errors:\n",
      "\n",
      "\n",
      "[3] GRAMMATICAL & OMISSION ERROR ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "Error Type Distribution:\n",
      "  Omission (missing words):      19\n",
      "  Insertion (extra words):       13\n",
      "  Substitution (wrong words):    3\n",
      "\n",
      "Examples - OMISSION (missing words):\n",
      "\n",
      "  Example 1:\n",
      "    REF:  Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen\n",
      "    PRED: eine gruppe männer männern, einen <unk> aus.\n",
      "    Missing: Baumwolle, von, Eine\n",
      "\n",
      "  Example 2:\n",
      "    REF:  Ein Mann schläft in einem grünen Raum auf einem Sofa.\n",
      "    PRED: ein mann schläft in einem grünen grünen auf einem grünen sofa.\n",
      "    Missing: Raum, Mann, Ein\n",
      "\n",
      "  Example 3:\n",
      "    REF:  Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau.\n",
      "    PRED: ein junge mit einer kamera auf einer schultern.\n",
      "    Missing: den, Kopfhörern, Junge\n",
      "\n",
      "Examples - INSERTION (extra words):\n",
      "\n",
      "  Example 1:\n",
      "    REF:  Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen\n",
      "    PRED: eine gruppe männer männern, einen <unk> aus.\n",
      "    Extra: männer, männern,, <unk>\n",
      "\n",
      "  Example 2:\n",
      "    REF:  Ein Mann schläft in einem grünen Raum auf einem Sofa.\n",
      "    PRED: ein mann schläft in einem grünen grünen auf einem grünen sofa.\n",
      "    Extra: ein, sofa., mann\n",
      "\n",
      "  Example 3:\n",
      "    REF:  Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau.\n",
      "    PRED: ein junge mit einer kamera auf einer schultern.\n",
      "    Extra: ein, schultern., kamera\n",
      "\n",
      "Examples - SUBSTITUTION (wrong words):\n",
      "\n",
      "  Example 1:\n",
      "    REF:  Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen\n",
      "    PRED: eine gruppe männer männern, einen <unk> aus.\n",
      "    Matched 1/9 tokens\n",
      "\n",
      "  Example 2:\n",
      "    REF:  Ein Mann schläft in einem grünen Raum auf einem Sofa.\n",
      "    PRED: ein mann schläft in einem grünen grünen auf einem grünen sofa.\n",
      "    Matched 5/10 tokens\n",
      "\n",
      "  Example 3:\n",
      "    REF:  Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau.\n",
      "    PRED: ein junge mit einer kamera auf einer schultern.\n",
      "    Matched 3/10 tokens\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PART 2: IMPROVEMENT PROPOSALS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "[IMPROVEMENT 1] ATTENTION MECHANISM\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "PROBLEM:\n",
      "  - Context vector (fixed size) từ encoder không thể lưu toàn bộ thông tin từ câu dài\n",
      "  - Decoder không biết từ nào trong input là quan trọng nhất\n",
      "  - Kết quả: dịch sai, thiếu từ, lỗi ngữ pháp\n",
      "\n",
      "SOLUTION: ADDITIVE ATTENTION (Bahdanau)\n",
      "  - Decoder tập trung (attend) vào các từ khác nhau của input tại mỗi bước\n",
      "  - Công thức: attention_weight = softmax(v^T * tanh(W_q*query + W_k*key))\n",
      "  - Query: hidden state của decoder\n",
      "  - Key: encoder outputs\n",
      "\n",
      "EXPECTED IMPROVEMENT:\n",
      "  - BLEU +5-10%\n",
      "  - Giảm lỗi thiếu từ\n",
      "  - Cải thiện câu dài\n",
      "\n",
      "\n",
      "IMPLEMENTATION - Attention Layer:\n",
      "\n",
      "class Attention(nn.Module):\n",
      "    def __init__(self, hidden_size):\n",
      "        super().__init__()\n",
      "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
      "        self.v = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, decoder_hidden, encoder_outputs):\n",
      "        # decoder_hidden: (batch, hidden)\n",
      "        # encoder_outputs: (batch, seq_len, hidden)\n",
      "\n",
      "        # Repeat decoder_hidden for all encoder steps\n",
      "        decoder_hidden_expanded = decoder_hidden.unsqueeze(1)  # (B, 1, H)\n",
      "\n",
      "        # Calculate attention scores\n",
      "        combined = torch.tanh(self.attn(torch.cat(\n",
      "            [encoder_outputs, decoder_hidden_expanded.expand_as(encoder_outputs)], 2\n",
      "        )))  # (B, seq_len, H)\n",
      "\n",
      "        scores = self.v(combined)  # (B, seq_len, 1)\n",
      "        attn_weights = torch.softmax(scores, dim=1)  # (B, seq_len, 1)\n",
      "\n",
      "        # Apply attention to encoder outputs\n",
      "        context = torch.sum(attn_weights * encoder_outputs, dim=1)  # (B, H)\n",
      "\n",
      "        return context, attn_weights\n",
      "\n",
      "\n",
      "DECODER with ATTENTION:\n",
      "\n",
      "class DecoderWithAttention(nn.Module):\n",
      "    def __init__(self, vocab_size, embed_dim=512, hidden_size=512, num_layers=2, dropout=0.3):\n",
      "        super().__init__()\n",
      "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=stoi_de[\"<pad>\"])\n",
      "        self.attention = Attention(hidden_size)\n",
      "\n",
      "        self.lstm = nn.LSTM(embed_dim + hidden_size, hidden_size, \n",
      "                           num_layers=num_layers, dropout=dropout, batch_first=True)\n",
      "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
      "\n",
      "    def forward(self, input_token, hidden, encoder_outputs):\n",
      "        embedded = self.embedding(input_token).unsqueeze(1)  # (B, 1, E)\n",
      "\n",
      "        context, attn_weights = self.attention(hidden[0][-1], encoder_outputs)  # (B, H)\n",
      "\n",
      "        # Concatenate embedding with context\n",
      "        input_combined = torch.cat([embedded, context.unsqueeze(1)], dim=-1)  # (B, 1, E+H)\n",
      "\n",
      "        output, hidden = self.lstm(input_combined, hidden)\n",
      "        logits = self.fc(output.squeeze(1))\n",
      "\n",
      "        return logits, hidden, attn_weights\n",
      "\n",
      "\n",
      "\n",
      "[IMPROVEMENT 2] BYTE-PAIR ENCODING (BPE)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "PROBLEM:\n",
      "  - Từ vựng là từ nguyên (word-level): 10,000 từ\n",
      "  - Từ mới/hiếm không có trong vocab → <unk>\n",
      "  - Không thể xử lý morphological variation (walked, walking, walks)\n",
      "\n",
      "SOLUTION: BYTE-PAIR ENCODING (BPE)\n",
      "  - Chia từ thành subword units\n",
      "  - Ví dụ: \"wordpiece\" → \"word\" + \"piece\"\n",
      "  - Vocab size: 32,000-50,000 subwords\n",
      "  - Có thể tạo ra từ mới từ subwords\n",
      "\n",
      "EXPECTED IMPROVEMENT:\n",
      "  - BLEU +3-7%\n",
      "  - Giảm <unk> tokens (~1-2% → 0.1-0.5%)\n",
      "  - Xử lý từ hiếm tốt hơn\n",
      "\n",
      "POPULAR LIBRARIES:\n",
      "  - sentencepiece: https://github.com/google/sentencepiece\n",
      "  - huggingface tokenizers: https://huggingface.co/docs/tokenizers/\n",
      "\n",
      "\n",
      "BPE EXAMPLE - Installation & Usage:\n",
      "\n",
      "# Installation\n",
      "pip install sentencepiece\n",
      "\n",
      "# Training BPE\n",
      "import sentencepiece as spm\n",
      "\n",
      "# Train BPE model on training corpus\n",
      "spm.SentencePieceTrainer.train(\n",
      "    input='train.en',  # Input text file\n",
      "    model_prefix='en_model',  # Output model prefix\n",
      "    vocab_size=32000,  # Vocabulary size\n",
      "    model_type='bpe',\n",
      "    normalization_rule_name='identity'\n",
      ")\n",
      "\n",
      "spm.SentencePieceTrainer.train(\n",
      "    input='train.de',\n",
      "    model_prefix='de_model',\n",
      "    vocab_size=32000,\n",
      "    model_type='bpe'\n",
      ")\n",
      "\n",
      "# Using BPE\n",
      "en_bpe = spm.SentencePieceProcessor(model_file='en_model.model')\n",
      "de_bpe = spm.SentencePieceProcessor(model_file='de_model.model')\n",
      "\n",
      "# Tokenize sentence\n",
      "en_sentence = \"The quick brown fox\"\n",
      "en_tokens = en_bpe.encode_as_pieces(en_sentence)\n",
      "# Output: ['▁The', '▁quick', '▁brown', '▁fox']\n",
      "\n",
      "# Encode to IDs\n",
      "en_ids = en_bpe.encode_as_ids(en_sentence)\n",
      "# Output: [47, 1234, 5678, 9012]\n",
      "\n",
      "# Decode back\n",
      "en_decoded = en_bpe.decode_ids(en_ids)\n",
      "# Output: \"The quick brown fox\"\n",
      "\n",
      "\n",
      "BPE vs WORD-LEVEL COMPARISON:\n",
      "\n",
      "Sentence: \"He walked quickly and ran.\"\n",
      "\n",
      "WORD-LEVEL:\n",
      "  Tokens: ['He', 'walked', 'quickly', 'and', 'ran', '.']\n",
      "  If 'walked' not in vocab → <unk>\n",
      "\n",
      "BPE (vocab_size=32k):\n",
      "  Tokens: ['He', '▁walk', 'ed', '▁quick', 'ly', '▁and', '▁ran', '.']\n",
      "  Can reconstruct 'walked' from subwords\n",
      "  'walked' appears in training → learned representation\n",
      "\n",
      "\n",
      "\n",
      "[IMPROVEMENT 3] BEAM SEARCH\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "PROBLEM:\n",
      "  - Greedy decoding: chọn token có xác suất cao nhất tại mỗi bước\n",
      "  - Không tối ưu toàn cục\n",
      "  - Có thể bỏ lỡ dịch tốt hơn\n",
      "\n",
      "SOLUTION: BEAM SEARCH\n",
      "  - Giữ K dòng dịch tốt nhất tại mỗi bước (K=beam_width)\n",
      "  - So sánh xác suất tích lũy\n",
      "  - Chọn K dòng có xác suất cao nhất\n",
      "\n",
      "EXPECTED IMPROVEMENT:\n",
      "  - BLEU +2-5% (compared to greedy)\n",
      "  - Dịch chất lượng cao hơn\n",
      "  - Trade-off: chậm hơn K lần\n",
      "\n",
      "BEAM WIDTHS:\n",
      "  - K=1: Greedy (tính cơ bản)\n",
      "  - K=3-5: Cân bằng tốt (recommended)\n",
      "  - K=10: Chất lượng cao nhưng chậm\n",
      "\n",
      "\n",
      "BEAM SEARCH IMPLEMENTATION:\n",
      "\n",
      "BEAM SEARCH EXAMPLE:\n",
      "\n",
      "# Initialize beam search decoder\n",
      "beam_decoder = BeamSearchDecoder(model, device, beam_width=5)\n",
      "\n",
      "# Translate with beam search\n",
      "en_sent = \"Hello, how are you?\"\n",
      "de_translation = beam_decoder.translate_beam_search(\n",
      "    en_sent, \n",
      "    tokenizer_en, stoi_en, stoi_de, itos_de\n",
      ")\n",
      "\n",
      "print(f\"EN: {en_sent}\")\n",
      "print(f\"DE: {de_translation}\")\n",
      "\n",
      "\n",
      "\n",
      "[IMPROVEMENT 4] COMPARISON & IMPLEMENTATION PRIORITY\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "   Technique Difficulty BLEU Gain  Speed Impact     Priority Implementation Time\n",
      "   Attention     Medium    +5-10% 10-20% slower 1 (Critical)           2-3 hours\n",
      "         BPE        Low     +3-7%          Same     2 (High)              30 min\n",
      " Beam Search     Medium     +2-5%  3-10x slower   3 (Medium)           1-2 hours\n",
      "Scheduled TF        Low     +1-2%          None 4 (Optional)              30 min\n",
      "    Ensemble       High     +3-5%    K x slower 5 (Advanced)           3-4 hours\n",
      "\n",
      "RECOMMENDED IMPLEMENTATION ORDER:\n",
      "  1. BPE (Quick win, easy to implement)\n",
      "  2. Attention (Biggest improvement on long sentences)\n",
      "  3. Beam Search (Polish results, good BLEU boost)\n",
      "  4. Scheduled Teacher Forcing (Fine-tuning)\n",
      "  5. Model Ensemble (If time permits)\n",
      "\n",
      "\n",
      "\n",
      "[IMPROVEMENT 5] QUICK WINS (Easy to implement now)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. INCREASE VOCAB SIZE\n",
      "   Before: 10,000 words\n",
      "   After: 20,000-30,000 words\n",
      "   Benefit: Fewer <unk> tokens\n",
      "   Implementation: 1 line change\n",
      "\n",
      "   Code:\n",
      "   vocab_en, stoi_en = build_vocab(train_en, tokenizer_en, max_words=20000)\n",
      "   vocab_de, stoi_de = build_vocab(train_de, tokenizer_de, max_words=20000)\n",
      "\n",
      "   Expected: +1-2% BLEU\n",
      "\n",
      "2. LAYER NORMALIZATION + RESIDUAL CONNECTIONS\n",
      "   Before: Basic LSTM layers\n",
      "   After: Add LayerNorm and skip connections\n",
      "   Benefit: Better gradient flow, faster training\n",
      "   Expected: +1-3% BLEU, faster convergence\n",
      "\n",
      "   Code:\n",
      "   class EncoderWithNorm(nn.Module):\n",
      "       def __init__(self, vocab_size, embed_dim=512, hidden_size=512, num_layers=2):\n",
      "           super().__init__()\n",
      "           self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
      "           self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers, batch_first=True)\n",
      "           self.norm = nn.LayerNorm(hidden_size)\n",
      "\n",
      "       def forward(self, src, src_lengths):\n",
      "           embedded = self.embedding(src)\n",
      "           packed = pack_padded_sequence(embedded, src_lengths.cpu(), batch_first=True)\n",
      "           outputs, hidden = self.lstm(packed)\n",
      "           outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
      "\n",
      "           # Apply layer normalization\n",
      "           outputs = self.norm(outputs)\n",
      "           return outputs, hidden\n",
      "\n",
      "3. INCREASE TRAINING EPOCHS (if early stopping not triggered)\n",
      "   Before: 10 epochs\n",
      "   After: 20-30 epochs (with early stopping=5)\n",
      "   Benefit: Model learns more patterns\n",
      "   Expected: +1-2% BLEU\n",
      "\n",
      "4. DROPOUT SCHEDULING\n",
      "   Before: Fixed dropout=0.3\n",
      "   After: Increase dropout gradually\n",
      "   Benefit: Regularization improves generalization\n",
      "\n",
      "   Code:\n",
      "   def adjust_dropout(epoch, max_epochs):\n",
      "       return 0.1 + (0.4 * epoch / max_epochs)\n",
      "\n",
      "   model.decoder.lstm.dropout = adjust_dropout(epoch, 20)\n",
      "\n",
      "5. WEIGHTED LOSS (penalize missing words more)\n",
      "   Before: Uniform loss weights\n",
      "   After: Weight rare words higher\n",
      "   Benefit: Model pays more attention to important words\n",
      "   Expected: +0.5-1% BLEU\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SUMMARY & RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "ROOT CAUSES OF ERRORS:\n",
      "  1. OOV (Out-of-Vocabulary) words → <unk> token\n",
      "  2. Long sentences → context vector overflow\n",
      "  3. Lack of attention → wrong focus\n",
      "  4. Greedy decoding → suboptimal translations\n",
      "\n",
      "IMMEDIATE ACTIONS (Today):\n",
      "  ✓ Increase vocab size: 10k → 20k\n",
      "  ✓ Try beam search with K=5\n",
      "  ✓ Analyze OOV impact on BLEU\n",
      "\n",
      "SHORT TERM (This week):\n",
      "  ✓ Implement BPE tokenization\n",
      "  ✓ Add attention mechanism\n",
      "  ✓ Fine-tune hyperparameters\n",
      "\n",
      "MEDIUM TERM (This sprint):\n",
      "  ✓ Implement scheduled teacher forcing\n",
      "  ✓ Add layer normalization\n",
      "  ✓ Consider multi-head attention\n",
      "\n",
      "LONG TERM (Next project):\n",
      "  ✓ Transformer-based models (BERT, mT5)\n",
      "  ✓ Pre-trained embeddings (fastText, mBERT)\n",
      "  ✓ Data augmentation\n",
      "  ✓ Back-translation for more training data\n",
      "  ✓ Model ensemble\n",
      "\n",
      "EXPECTED FINAL IMPROVEMENTS:\n",
      "  Before:  BLEU ≈ X.XX%\n",
      "  BPE:     BLEU ≈ (X + 3-5)%\n",
      "  + Attn:  BLEU ≈ (X + 8-15)%\n",
      "  + Beam:  BLEU ≈ (X + 10-20)%\n",
      "  Total:   BLEU improvement: 10-20%\n",
      "\n",
      "\n",
      "================================================================================\n",
      "END OF ERROR ANALYSIS & IMPROVEMENTS\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 12. PHÂN TÍCH LỖI VÀ ĐỀ XUẤT CẢI TIẾN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"12. ERROR ANALYSIS & IMPROVEMENT PROPOSALS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== PHẦN 1: Phân tích chi tiết các lỗi phổ biến ==========\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"PART 1: COMMON ERRORS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def analyze_oov_errors(test_en, test_de, predictions, tokenizer_en, tokenizer_de, stoi_en, stoi_de):\n",
    "    \"\"\"\n",
    "    Phân tích lỗi từ hiếm (Out-of-Vocabulary - OOV)\n",
    "    \"\"\"\n",
    "    print(\"\\n[1] OUT-OF-VOCABULARY (OOV) ERROR ANALYSIS\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    oov_counts_en = []\n",
    "    oov_counts_de = []\n",
    "    oov_in_error = 0\n",
    "    total_errors = 0\n",
    "    \n",
    "    examples_with_oov = []\n",
    "    \n",
    "    for idx, (en, de, pred) in enumerate(zip(test_en, test_de, predictions)):\n",
    "        # Đếm OOV tokens trong English\n",
    "        en_tokens = [t.text.lower() for t in tokenizer_en(en)]\n",
    "        oov_en = sum(1 for tok in en_tokens if tok not in stoi_en or stoi_en[tok] == stoi_en[\"<unk>\"])\n",
    "        oov_counts_en.append(oov_en)\n",
    "        \n",
    "        # Đếm OOV tokens trong German reference\n",
    "        de_tokens = [t.text.lower() for t in tokenizer_de(de)]\n",
    "        oov_de = sum(1 for tok in de_tokens if tok not in stoi_de or stoi_de[tok] == stoi_de[\"<unk>\"])\n",
    "        oov_counts_de.append(oov_de)\n",
    "        \n",
    "        # Nếu câu dự đoán khác với reference → lỗi\n",
    "        if pred != de:\n",
    "            total_errors += 1\n",
    "            if oov_en > 0 or oov_de > 0:\n",
    "                oov_in_error += 1\n",
    "                if len(examples_with_oov) < 5:\n",
    "                    examples_with_oov.append({\n",
    "                        'en': en,\n",
    "                        'de': de,\n",
    "                        'pred': pred,\n",
    "                        'oov_en': oov_en,\n",
    "                        'oov_de': oov_de\n",
    "                    })\n",
    "    \n",
    "    oov_en_total = sum(oov_counts_en)\n",
    "    oov_de_total = sum(oov_counts_de)\n",
    "    \n",
    "    print(f\"English OOV Statistics:\")\n",
    "    print(f\"  Total OOV tokens: {oov_en_total}\")\n",
    "    print(f\"  Avg OOV per sentence: {np.mean(oov_counts_en):.2f}\")\n",
    "    print(f\"  Max OOV in a sentence: {max(oov_counts_en)}\")\n",
    "    print(f\"  Sentences with OOV: {sum(1 for c in oov_counts_en if c > 0)}\")\n",
    "    \n",
    "    print(f\"\\nGerman OOV Statistics:\")\n",
    "    print(f\"  Total OOV tokens: {oov_de_total}\")\n",
    "    print(f\"  Avg OOV per sentence: {np.mean(oov_counts_de):.2f}\")\n",
    "    print(f\"  Max OOV in a sentence: {max(oov_counts_de)}\")\n",
    "    print(f\"  Sentences with OOV: {sum(1 for c in oov_counts_de if c > 0)}\")\n",
    "    \n",
    "    if total_errors > 0:\n",
    "        oov_error_pct = 100 * oov_in_error / total_errors\n",
    "        print(f\"\\nOOV Impact on Errors:\")\n",
    "        print(f\"  Errors with OOV: {oov_in_error} / {total_errors} ({oov_error_pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nExamples of OOV-related errors:\")\n",
    "    for i, ex in enumerate(examples_with_oov, 1):\n",
    "        print(f\"\\n  Example {i}:\")\n",
    "        print(f\"    EN (OOV: {ex['oov_en']}): {ex['en']}\")\n",
    "        print(f\"    DE ref (OOV: {ex['oov_de']}): {ex['de']}\")\n",
    "        print(f\"    DE pred: {ex['pred']}\")\n",
    "    \n",
    "    return oov_counts_en, oov_counts_de\n",
    "\n",
    "\n",
    "def analyze_length_errors(test_en, test_de, predictions):\n",
    "    \"\"\"\n",
    "    Phân tích lỗi do câu quá dài → mất thông tin\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n[2] LONG SENTENCE ERROR ANALYSIS\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    en_lengths = []\n",
    "    errors_by_length = {}\n",
    "    \n",
    "    for en, de, pred in zip(test_en, test_de, predictions):\n",
    "        en_len = len(en.split())\n",
    "        en_lengths.append(en_len)\n",
    "        \n",
    "        # Phân loại lỗi theo độ dài\n",
    "        length_bracket = (en_len // 5) * 5  # Nhóm theo 5 tokens\n",
    "        \n",
    "        if pred != de:\n",
    "            if length_bracket not in errors_by_length:\n",
    "                errors_by_length[length_bracket] = {\"total\": 0, \"errors\": 0}\n",
    "            errors_by_length[length_bracket][\"total\"] += 1\n",
    "            errors_by_length[length_bracket][\"errors\"] += 1\n",
    "        else:\n",
    "            if length_bracket not in errors_by_length:\n",
    "                errors_by_length[length_bracket] = {\"total\": 0, \"errors\": 0}\n",
    "            errors_by_length[length_bracket][\"total\"] += 1\n",
    "    \n",
    "    print(\"Error Rate by Sentence Length:\")\n",
    "    print(f\"{'Length Range':<20} {'Total':<10} {'Errors':<10} {'Error %':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for length in sorted(errors_by_length.keys()):\n",
    "        data = errors_by_length[length]\n",
    "        error_rate = 100 * data[\"errors\"] / data[\"total\"] if data[\"total\"] > 0 else 0\n",
    "        print(f\"{length}-{length+4:<16} {data['total']:<10} {data['errors']:<10} {error_rate:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nSentence Length Statistics:\")\n",
    "    print(f\"  Min length: {min(en_lengths)}\")\n",
    "    print(f\"  Max length: {max(en_lengths)}\")\n",
    "    print(f\"  Mean: {np.mean(en_lengths):.1f}\")\n",
    "    print(f\"  Median: {np.median(en_lengths):.1f}\")\n",
    "    \n",
    "    # Lấy ví dụ câu dài bị dịch sai\n",
    "    print(f\"\\nExamples of long sentences with errors:\")\n",
    "    long_error_examples = []\n",
    "    for en, de, pred in zip(test_en, test_de, predictions):\n",
    "        if len(en.split()) > 30 and pred != de:\n",
    "            long_error_examples.append((en, de, pred))\n",
    "            if len(long_error_examples) >= 3:\n",
    "                break\n",
    "    \n",
    "    for i, (en, de, pred) in enumerate(long_error_examples, 1):\n",
    "        print(f\"\\n  Example {i} (length: {len(en.split())}):\")\n",
    "        print(f\"    EN:   {en}\")\n",
    "        print(f\"    REF:  {de}\")\n",
    "        print(f\"    PRED: {pred}\")\n",
    "\n",
    "\n",
    "def analyze_grammatical_errors(test_de, predictions, tokenizer_de):\n",
    "    \"\"\"\n",
    "    Phân tích lỗi ngữ pháp và từ bị thiếu\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n[3] GRAMMATICAL & OMISSION ERROR ANALYSIS\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    omission_errors = 0\n",
    "    substitution_errors = 0\n",
    "    insertion_errors = 0\n",
    "    reordering_errors = 0\n",
    "    \n",
    "    examples = {\n",
    "        'omission': [],\n",
    "        'substitution': [],\n",
    "        'insertion': []\n",
    "    }\n",
    "    \n",
    "    for ref, pred in zip(test_de, predictions):\n",
    "        ref_tokens = set(ref.split())\n",
    "        pred_tokens = set(pred.split())\n",
    "        \n",
    "        ref_len = len(ref.split())\n",
    "        pred_len = len(pred.split())\n",
    "        \n",
    "        # Omission: từ bị thiếu\n",
    "        missing = ref_tokens - pred_tokens\n",
    "        if missing and len(examples['omission']) < 3:\n",
    "            examples['omission'].append({\n",
    "                'ref': ref,\n",
    "                'pred': pred,\n",
    "                'missing': list(missing)[:3]\n",
    "            })\n",
    "            omission_errors += len(missing)\n",
    "        \n",
    "        # Insertion: từ thừa\n",
    "        extra = pred_tokens - ref_tokens\n",
    "        if extra and len(examples['insertion']) < 3:\n",
    "            examples['insertion'].append({\n",
    "                'ref': ref,\n",
    "                'pred': pred,\n",
    "                'extra': list(extra)[:3]\n",
    "            })\n",
    "            insertion_errors += len(extra)\n",
    "        \n",
    "        # Substitution: từ sai\n",
    "        common = ref_tokens & pred_tokens\n",
    "        if len(common) < min(ref_len, pred_len) and len(examples['substitution']) < 3:\n",
    "            examples['substitution'].append({\n",
    "                'ref': ref,\n",
    "                'pred': pred,\n",
    "                'matched': len(common),\n",
    "                'ref_len': ref_len\n",
    "            })\n",
    "            substitution_errors += 1\n",
    "    \n",
    "    print(\"Error Type Distribution:\")\n",
    "    print(f\"  Omission (missing words):      {omission_errors}\")\n",
    "    print(f\"  Insertion (extra words):       {insertion_errors}\")\n",
    "    print(f\"  Substitution (wrong words):    {substitution_errors}\")\n",
    "    \n",
    "    print(f\"\\nExamples - OMISSION (missing words):\")\n",
    "    for i, ex in enumerate(examples['omission'], 1):\n",
    "        print(f\"\\n  Example {i}:\")\n",
    "        print(f\"    REF:  {ex['ref']}\")\n",
    "        print(f\"    PRED: {ex['pred']}\")\n",
    "        print(f\"    Missing: {', '.join(ex['missing'])}\")\n",
    "    \n",
    "    print(f\"\\nExamples - INSERTION (extra words):\")\n",
    "    for i, ex in enumerate(examples['insertion'], 1):\n",
    "        print(f\"\\n  Example {i}:\")\n",
    "        print(f\"    REF:  {ex['ref']}\")\n",
    "        print(f\"    PRED: {ex['pred']}\")\n",
    "        print(f\"    Extra: {', '.join(ex['extra'])}\")\n",
    "    \n",
    "    print(f\"\\nExamples - SUBSTITUTION (wrong words):\")\n",
    "    for i, ex in enumerate(examples['substitution'], 1):\n",
    "        print(f\"\\n  Example {i}:\")\n",
    "        print(f\"    REF:  {ex['ref']}\")\n",
    "        print(f\"    PRED: {ex['pred']}\")\n",
    "        print(f\"    Matched {ex['matched']}/{ex['ref_len']} tokens\")\n",
    "\n",
    "\n",
    "# Chạy phân tích chi tiết\n",
    "oov_en_counts, oov_de_counts = analyze_oov_errors(test_en, test_de, predictions, \n",
    "                                                    tokenizer_en, tokenizer_de, stoi_en, stoi_de)\n",
    "analyze_length_errors(test_en, test_de, predictions)\n",
    "analyze_grammatical_errors(test_de, predictions, tokenizer_de)\n",
    "\n",
    "\n",
    "# ========== PHẦN 2: Đề xuất cải tiến ==========\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"PART 2: IMPROVEMENT PROPOSALS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== Cải tiến 1: ATTENTION MECHANISM ==========\n",
    "\n",
    "print(\"\\n\\n[IMPROVEMENT 1] ATTENTION MECHANISM\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "PROBLEM:\n",
    "  - Context vector (fixed size) từ encoder không thể lưu toàn bộ thông tin từ câu dài\n",
    "  - Decoder không biết từ nào trong input là quan trọng nhất\n",
    "  - Kết quả: dịch sai, thiếu từ, lỗi ngữ pháp\n",
    "\n",
    "SOLUTION: ADDITIVE ATTENTION (Bahdanau)\n",
    "  - Decoder tập trung (attend) vào các từ khác nhau của input tại mỗi bước\n",
    "  - Công thức: attention_weight = softmax(v^T * tanh(W_q*query + W_k*key))\n",
    "  - Query: hidden state của decoder\n",
    "  - Key: encoder outputs\n",
    "  \n",
    "EXPECTED IMPROVEMENT:\n",
    "  - BLEU +5-10%\n",
    "  - Giảm lỗi thiếu từ\n",
    "  - Cải thiện câu dài\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nIMPLEMENTATION - Attention Layer:\")\n",
    "print(\"\"\"\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # decoder_hidden: (batch, hidden)\n",
    "        # encoder_outputs: (batch, seq_len, hidden)\n",
    "        \n",
    "        # Repeat decoder_hidden for all encoder steps\n",
    "        decoder_hidden_expanded = decoder_hidden.unsqueeze(1)  # (B, 1, H)\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        combined = torch.tanh(self.attn(torch.cat(\n",
    "            [encoder_outputs, decoder_hidden_expanded.expand_as(encoder_outputs)], 2\n",
    "        )))  # (B, seq_len, H)\n",
    "        \n",
    "        scores = self.v(combined)  # (B, seq_len, 1)\n",
    "        attn_weights = torch.softmax(scores, dim=1)  # (B, seq_len, 1)\n",
    "        \n",
    "        # Apply attention to encoder outputs\n",
    "        context = torch.sum(attn_weights * encoder_outputs, dim=1)  # (B, H)\n",
    "        \n",
    "        return context, attn_weights\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nDECODER with ATTENTION:\")\n",
    "print(\"\"\"\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, hidden_size=512, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=stoi_de[\"<pad>\"])\n",
    "        self.attention = Attention(hidden_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_dim + hidden_size, hidden_size, \n",
    "                           num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, input_token, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_token).unsqueeze(1)  # (B, 1, E)\n",
    "        \n",
    "        context, attn_weights = self.attention(hidden[0][-1], encoder_outputs)  # (B, H)\n",
    "        \n",
    "        # Concatenate embedding with context\n",
    "        input_combined = torch.cat([embedded, context.unsqueeze(1)], dim=-1)  # (B, 1, E+H)\n",
    "        \n",
    "        output, hidden = self.lstm(input_combined, hidden)\n",
    "        logits = self.fc(output.squeeze(1))\n",
    "        \n",
    "        return logits, hidden, attn_weights\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ========== Cải tiến 2: BYTE-PAIR ENCODING (BPE) ==========\n",
    "\n",
    "print(\"\\n\\n[IMPROVEMENT 2] BYTE-PAIR ENCODING (BPE)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "PROBLEM:\n",
    "  - Từ vựng là từ nguyên (word-level): 10,000 từ\n",
    "  - Từ mới/hiếm không có trong vocab → <unk>\n",
    "  - Không thể xử lý morphological variation (walked, walking, walks)\n",
    "\n",
    "SOLUTION: BYTE-PAIR ENCODING (BPE)\n",
    "  - Chia từ thành subword units\n",
    "  - Ví dụ: \"wordpiece\" → \"word\" + \"piece\"\n",
    "  - Vocab size: 32,000-50,000 subwords\n",
    "  - Có thể tạo ra từ mới từ subwords\n",
    "  \n",
    "EXPECTED IMPROVEMENT:\n",
    "  - BLEU +3-7%\n",
    "  - Giảm <unk> tokens (~1-2% → 0.1-0.5%)\n",
    "  - Xử lý từ hiếm tốt hơn\n",
    "  \n",
    "POPULAR LIBRARIES:\n",
    "  - sentencepiece: https://github.com/google/sentencepiece\n",
    "  - huggingface tokenizers: https://huggingface.co/docs/tokenizers/\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nBPE EXAMPLE - Installation & Usage:\")\n",
    "print(\"\"\"\n",
    "# Installation\n",
    "pip install sentencepiece\n",
    "\n",
    "# Training BPE\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Train BPE model on training corpus\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='train.en',  # Input text file\n",
    "    model_prefix='en_model',  # Output model prefix\n",
    "    vocab_size=32000,  # Vocabulary size\n",
    "    model_type='bpe',\n",
    "    normalization_rule_name='identity'\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='train.de',\n",
    "    model_prefix='de_model',\n",
    "    vocab_size=32000,\n",
    "    model_type='bpe'\n",
    ")\n",
    "\n",
    "# Using BPE\n",
    "en_bpe = spm.SentencePieceProcessor(model_file='en_model.model')\n",
    "de_bpe = spm.SentencePieceProcessor(model_file='de_model.model')\n",
    "\n",
    "# Tokenize sentence\n",
    "en_sentence = \"The quick brown fox\"\n",
    "en_tokens = en_bpe.encode_as_pieces(en_sentence)\n",
    "# Output: ['▁The', '▁quick', '▁brown', '▁fox']\n",
    "\n",
    "# Encode to IDs\n",
    "en_ids = en_bpe.encode_as_ids(en_sentence)\n",
    "# Output: [47, 1234, 5678, 9012]\n",
    "\n",
    "# Decode back\n",
    "en_decoded = en_bpe.decode_ids(en_ids)\n",
    "# Output: \"The quick brown fox\"\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nBPE vs WORD-LEVEL COMPARISON:\")\n",
    "print(\"\"\"\n",
    "Sentence: \"He walked quickly and ran.\"\n",
    "\n",
    "WORD-LEVEL:\n",
    "  Tokens: ['He', 'walked', 'quickly', 'and', 'ran', '.']\n",
    "  If 'walked' not in vocab → <unk>\n",
    "\n",
    "BPE (vocab_size=32k):\n",
    "  Tokens: ['He', '▁walk', 'ed', '▁quick', 'ly', '▁and', '▁ran', '.']\n",
    "  Can reconstruct 'walked' from subwords\n",
    "  'walked' appears in training → learned representation\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ========== Cải tiến 3: BEAM SEARCH ==========\n",
    "\n",
    "print(\"\\n\\n[IMPROVEMENT 3] BEAM SEARCH\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "PROBLEM:\n",
    "  - Greedy decoding: chọn token có xác suất cao nhất tại mỗi bước\n",
    "  - Không tối ưu toàn cục\n",
    "  - Có thể bỏ lỡ dịch tốt hơn\n",
    "\n",
    "SOLUTION: BEAM SEARCH\n",
    "  - Giữ K dòng dịch tốt nhất tại mỗi bước (K=beam_width)\n",
    "  - So sánh xác suất tích lũy\n",
    "  - Chọn K dòng có xác suất cao nhất\n",
    "  \n",
    "EXPECTED IMPROVEMENT:\n",
    "  - BLEU +2-5% (compared to greedy)\n",
    "  - Dịch chất lượng cao hơn\n",
    "  - Trade-off: chậm hơn K lần\n",
    "  \n",
    "BEAM WIDTHS:\n",
    "  - K=1: Greedy (tính cơ bản)\n",
    "  - K=3-5: Cân bằng tốt (recommended)\n",
    "  - K=10: Chất lượng cao nhưng chậm\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nBEAM SEARCH IMPLEMENTATION:\")\n",
    "\n",
    "class BeamSearchDecoder:\n",
    "    def __init__(self, model, device, max_length=50, beam_width=5):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.beam_width = beam_width\n",
    "    \n",
    "    def translate_beam_search(self, sentence, tokenizer_en, stoi_en, stoi_de, itos_de):\n",
    "        \"\"\"\n",
    "        Beam search translation\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Encode input\n",
    "        tokens_en = [t.text.lower() for t in tokenizer_en(sentence)]\n",
    "        input_ids = [stoi_en.get(\"<sos>\", 1)] + \\\n",
    "                   [stoi_en.get(tok, stoi_en[\"<unk>\"]) for tok in tokens_en] + \\\n",
    "                   [stoi_en.get(\"<eos>\", 3)]\n",
    "        src_tensor = torch.tensor(input_ids).unsqueeze(0).to(self.device)\n",
    "        src_length = torch.tensor([len(input_ids)]).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Encoder\n",
    "            _, hidden = self.model.encoder(src_tensor, src_length)\n",
    "            \n",
    "            # Beam search\n",
    "            beams = [{'tokens': [stoi_de[\"<sos>\"]], 'hidden': hidden, 'score': 0.0}]\n",
    "            \n",
    "            for step in range(1, self.max_length):\n",
    "                candidates = []\n",
    "                \n",
    "                for beam in beams:\n",
    "                    if beam['tokens'][-1] == stoi_de[\"<eos>\"]:\n",
    "                        candidates.append(beam)\n",
    "                        continue\n",
    "                    \n",
    "                    input_token = torch.tensor([beam['tokens'][-1]]).to(self.device)\n",
    "                    logits, next_hidden = self.model.decoder(input_token, beam['hidden'])\n",
    "                    \n",
    "                    # Get top K probabilities\n",
    "                    log_probs = torch.log_softmax(logits, dim=-1)[0]\n",
    "                    top_k_probs, top_k_ids = torch.topk(log_probs, self.beam_width)\n",
    "                    \n",
    "                    for prob, token_id in zip(top_k_probs, top_k_ids):\n",
    "                        new_beam = {\n",
    "                            'tokens': beam['tokens'] + [token_id.item()],\n",
    "                            'hidden': next_hidden,\n",
    "                            'score': beam['score'] + prob.item()\n",
    "                        }\n",
    "                        candidates.append(new_beam)\n",
    "                \n",
    "                # Sort by score and keep top K\n",
    "                candidates = sorted(candidates, key=lambda x: x['score'], reverse=True)\n",
    "                beams = candidates[:self.beam_width]\n",
    "                \n",
    "                # Check if all beams ended\n",
    "                if all(b['tokens'][-1] == stoi_de[\"<eos>\"] for b in beams):\n",
    "                    break\n",
    "            \n",
    "            # Get best translation\n",
    "            best_beam = beams[0]\n",
    "            output_ids = best_beam['tokens'][1:]  # Remove <sos>\n",
    "            \n",
    "            # Detokenize\n",
    "            output_tokens = [itos_de.get(idx, \"<unk>\") for idx in output_ids]\n",
    "            if output_tokens and output_tokens[-1] == \"<eos>\":\n",
    "                output_tokens = output_tokens[:-1]\n",
    "            \n",
    "            return \" \".join(output_tokens)\n",
    "\n",
    "print(\"\\nBEAM SEARCH EXAMPLE:\")\n",
    "print(\"\"\"\n",
    "# Initialize beam search decoder\n",
    "beam_decoder = BeamSearchDecoder(model, device, beam_width=5)\n",
    "\n",
    "# Translate with beam search\n",
    "en_sent = \"Hello, how are you?\"\n",
    "de_translation = beam_decoder.translate_beam_search(\n",
    "    en_sent, \n",
    "    tokenizer_en, stoi_en, stoi_de, itos_de\n",
    ")\n",
    "\n",
    "print(f\"EN: {en_sent}\")\n",
    "print(f\"DE: {de_translation}\")\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ========== Cải tiến 4: COMPARISON TABLE ==========\n",
    "\n",
    "print(\"\\n\\n[IMPROVEMENT 4] COMPARISON & IMPLEMENTATION PRIORITY\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "comparison_data = {\n",
    "    'Technique': ['Attention', 'BPE', 'Beam Search', 'Scheduled TF', 'Ensemble'],\n",
    "    'Difficulty': ['Medium', 'Low', 'Medium', 'Low', 'High'],\n",
    "    'BLEU Gain': ['+5-10%', '+3-7%', '+2-5%', '+1-2%', '+3-5%'],\n",
    "    'Speed Impact': ['10-20% slower', 'Same', '3-10x slower', 'None', 'K x slower'],\n",
    "    'Priority': ['1 (Critical)', '2 (High)', '3 (Medium)', '4 (Optional)', '5 (Advanced)'],\n",
    "    'Implementation Time': ['2-3 hours', '30 min', '1-2 hours', '30 min', '3-4 hours']\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + df_comparison.to_string(index=False))\n",
    "\n",
    "print(\"\"\"\n",
    "RECOMMENDED IMPLEMENTATION ORDER:\n",
    "  1. BPE (Quick win, easy to implement)\n",
    "  2. Attention (Biggest improvement on long sentences)\n",
    "  3. Beam Search (Polish results, good BLEU boost)\n",
    "  4. Scheduled Teacher Forcing (Fine-tuning)\n",
    "  5. Model Ensemble (If time permits)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ========== Cải tiến 5: QUICK WINS (EASY IMPROVEMENTS) ==========\n",
    "\n",
    "print(\"\\n\\n[IMPROVEMENT 5] QUICK WINS (Easy to implement now)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "1. INCREASE VOCAB SIZE\n",
    "   Before: 10,000 words\n",
    "   After: 20,000-30,000 words\n",
    "   Benefit: Fewer <unk> tokens\n",
    "   Implementation: 1 line change\n",
    "   \n",
    "   Code:\n",
    "   vocab_en, stoi_en = build_vocab(train_en, tokenizer_en, max_words=20000)\n",
    "   vocab_de, stoi_de = build_vocab(train_de, tokenizer_de, max_words=20000)\n",
    "   \n",
    "   Expected: +1-2% BLEU\n",
    "\n",
    "2. LAYER NORMALIZATION + RESIDUAL CONNECTIONS\n",
    "   Before: Basic LSTM layers\n",
    "   After: Add LayerNorm and skip connections\n",
    "   Benefit: Better gradient flow, faster training\n",
    "   Expected: +1-3% BLEU, faster convergence\n",
    "   \n",
    "   Code:\n",
    "   class EncoderWithNorm(nn.Module):\n",
    "       def __init__(self, vocab_size, embed_dim=512, hidden_size=512, num_layers=2):\n",
    "           super().__init__()\n",
    "           self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "           self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers, batch_first=True)\n",
    "           self.norm = nn.LayerNorm(hidden_size)\n",
    "       \n",
    "       def forward(self, src, src_lengths):\n",
    "           embedded = self.embedding(src)\n",
    "           packed = pack_padded_sequence(embedded, src_lengths.cpu(), batch_first=True)\n",
    "           outputs, hidden = self.lstm(packed)\n",
    "           outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
    "           \n",
    "           # Apply layer normalization\n",
    "           outputs = self.norm(outputs)\n",
    "           return outputs, hidden\n",
    "\n",
    "3. INCREASE TRAINING EPOCHS (if early stopping not triggered)\n",
    "   Before: 10 epochs\n",
    "   After: 20-30 epochs (with early stopping=5)\n",
    "   Benefit: Model learns more patterns\n",
    "   Expected: +1-2% BLEU\n",
    "\n",
    "4. DROPOUT SCHEDULING\n",
    "   Before: Fixed dropout=0.3\n",
    "   After: Increase dropout gradually\n",
    "   Benefit: Regularization improves generalization\n",
    "   \n",
    "   Code:\n",
    "   def adjust_dropout(epoch, max_epochs):\n",
    "       return 0.1 + (0.4 * epoch / max_epochs)\n",
    "   \n",
    "   model.decoder.lstm.dropout = adjust_dropout(epoch, 20)\n",
    "\n",
    "5. WEIGHTED LOSS (penalize missing words more)\n",
    "   Before: Uniform loss weights\n",
    "   After: Weight rare words higher\n",
    "   Benefit: Model pays more attention to important words\n",
    "   Expected: +0.5-1% BLEU\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ========== Summary & Recommendations ==========\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "ROOT CAUSES OF ERRORS:\n",
    "  1. OOV (Out-of-Vocabulary) words → <unk> token\n",
    "  2. Long sentences → context vector overflow\n",
    "  3. Lack of attention → wrong focus\n",
    "  4. Greedy decoding → suboptimal translations\n",
    "\n",
    "IMMEDIATE ACTIONS (Today):\n",
    "  ✓ Increase vocab size: 10k → 20k\n",
    "  ✓ Try beam search with K=5\n",
    "  ✓ Analyze OOV impact on BLEU\n",
    "\n",
    "SHORT TERM (This week):\n",
    "  ✓ Implement BPE tokenization\n",
    "  ✓ Add attention mechanism\n",
    "  ✓ Fine-tune hyperparameters\n",
    "\n",
    "MEDIUM TERM (This sprint):\n",
    "  ✓ Implement scheduled teacher forcing\n",
    "  ✓ Add layer normalization\n",
    "  ✓ Consider multi-head attention\n",
    "\n",
    "LONG TERM (Next project):\n",
    "  ✓ Transformer-based models (BERT, mT5)\n",
    "  ✓ Pre-trained embeddings (fastText, mBERT)\n",
    "  ✓ Data augmentation\n",
    "  ✓ Back-translation for more training data\n",
    "  ✓ Model ensemble\n",
    "\n",
    "EXPECTED FINAL IMPROVEMENTS:\n",
    "  Before:  BLEU ≈ X.XX%\n",
    "  BPE:     BLEU ≈ (X + 3-5)%\n",
    "  + Attn:  BLEU ≈ (X + 8-15)%\n",
    "  + Beam:  BLEU ≈ (X + 10-20)%\n",
    "  Total:   BLEU improvement: 10-20%\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"END OF ERROR ANALYSIS & IMPROVEMENTS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a814bd",
   "metadata": {},
   "source": [
    "# CHECKPOINT MANGEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e1062144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CHECKPOINT MANAGEMENT\n",
      "================================================================================\n",
      "\n",
      "[1] CHECKPOINT CONFIGURATION\n",
      "--------------------------------------------------------------------------------\n",
      "✓ CheckpointManager initialized\n",
      "\n",
      "\n",
      "[2] UPDATED TRAINING LOOP WITH CHECKPOINT\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "# Thêm vào training loop (phần 8):\n",
      "\n",
      "for epoch in range(1, NUM_EPOCHS + 1):\n",
      "    start_time = time.time()\n",
      "    model.train()\n",
      "    train_loss = 0.0\n",
      "    n_batches = 0\n",
      "\n",
      "    for src, src_lengths, trg, trg_lengths in train_loader:\n",
      "        src = src.to(device)\n",
      "        src_lengths = src_lengths.to(device)\n",
      "        trg = trg.to(device)\n",
      "\n",
      "        optimizer.zero_grad()\n",
      "        outputs = model(src, src_lengths, trg)\n",
      "        vocab_size = outputs.size(-1)\n",
      "\n",
      "        pred = outputs[:, 1:, :].contiguous().view(-1, vocab_size)\n",
      "        target = trg[:, 1:].contiguous().view(-1)\n",
      "\n",
      "        loss = criterion(pred, target)\n",
      "        loss.backward()\n",
      "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
      "        optimizer.step()\n",
      "\n",
      "        train_loss += loss.item()\n",
      "        n_batches += 1\n",
      "\n",
      "    avg_train_loss = train_loss / (n_batches if n_batches > 0 else 1)\n",
      "    avg_val_loss = evaluate(model, val_loader, criterion, device)\n",
      "\n",
      "    history[\"train_loss\"].append(avg_train_loss)\n",
      "    history[\"val_loss\"].append(avg_val_loss)\n",
      "\n",
      "    if USE_SCHEDULER:\n",
      "        scheduler.step(avg_val_loss)\n",
      "\n",
      "    # ========== CHECKPOINT SAVING ==========\n",
      "    if avg_val_loss < best_val_loss:\n",
      "        best_val_loss = avg_val_loss\n",
      "        epochs_no_improve = 0\n",
      "        best_note = \" (best -> saved)\"\n",
      "\n",
      "        # LƯU CHECKPOINT\n",
      "        checkpoint_manager.save_checkpoint(\n",
      "            model, optimizer, epoch, avg_val_loss, is_best=True\n",
      "        )\n",
      "    else:\n",
      "        epochs_no_improve += 1\n",
      "        best_note = \"\"\n",
      "\n",
      "        # LƯU CHECKPOINT THƯỜNG XUYÊN (mỗi 5 epoch)\n",
      "        if epoch % 5 == 0:\n",
      "            checkpoint_manager.save_checkpoint(\n",
      "                model, optimizer, epoch, avg_val_loss, is_best=False\n",
      "            )\n",
      "\n",
      "    elapsed = time.time() - start_time\n",
      "    print(f\"Epoch {epoch:02d} | Train loss: {avg_train_loss:.4f} | Val loss: {avg_val_loss:.4f}{best_note} | Time: {elapsed:.1f}s\")\n",
      "\n",
      "    if epochs_no_improve >= PATIENCE:\n",
      "        print(f\"Early stopping triggered. No improvement for {PATIENCE} epochs.\")\n",
      "        break\n",
      "\n",
      "print(f\"Training finished. Best val loss: {best_val_loss:.4f}\")\n",
      "\n",
      "# Liệt kê tất cả checkpoints\n",
      "checkpoint_manager.list_checkpoints()\n",
      "\n",
      "\n",
      "\n",
      "[3] RESUME TRAINING FROM CHECKPOINT\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "# Ví dụ: Khôi phục training từ checkpoint\n",
      "\n",
      "# Bước 1: Tạo model mới\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "encoder = Encoder(\n",
      "    vocab_size=len(vocab_en),\n",
      "    embed_dim=512,\n",
      "    hidden_size=512,\n",
      "    num_layers=2,\n",
      "    dropout=0.3\n",
      ")\n",
      "\n",
      "decoder = Decoder(\n",
      "    vocab_size=len(vocab_de),\n",
      "    embed_dim=512,\n",
      "    hidden_size=512,\n",
      "    num_layers=2,\n",
      "    dropout=0.3\n",
      ")\n",
      "\n",
      "model = Seq2Seq(encoder, decoder, device).to(device)\n",
      "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
      "\n",
      "# Bước 2: Tải checkpoint\n",
      "checkpoint_path = \"./checkpoints/checkpoint_epoch_010_loss_3.2456.pt\"\n",
      "start_epoch, best_val_loss = checkpoint_manager.load_checkpoint(\n",
      "    checkpoint_path, model, optimizer, device\n",
      ")\n",
      "\n",
      "# Bước 3: Tiếp tục training từ epoch sau đó\n",
      "for epoch in range(start_epoch + 1, NUM_EPOCHS + 1):\n",
      "    # ... training code ...\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "[4] INFERENCE FROM CHECKPOINT\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "# Sử dụng best model để inference\n",
      "\n",
      "# Tải best model\n",
      "model_inference = Seq2Seq(encoder, decoder, device).to(device)\n",
      "checkpoint_manager.load_best_model(model_inference, device)\n",
      "\n",
      "# Dịch câu\n",
      "test_sentences = [\n",
      "    \"Hello, how are you?\",\n",
      "    \"What is your name?\",\n",
      "    \"The weather is nice today.\"\n",
      "]\n",
      "\n",
      "print(\"\\n\" + \"=\"*60)\n",
      "print(\"INFERENCE USING BEST MODEL\")\n",
      "print(\"=\"*60)\n",
      "\n",
      "for en_sent in test_sentences:\n",
      "    de_sent = translate(en_sent, model_inference, device, \n",
      "                       tokenizer_en, stoi_en, itos_de, stoi_de)\n",
      "    print(f\"EN: {en_sent}\")\n",
      "    print(f\"DE: {de_sent}\")\n",
      "    print()\n",
      "\n",
      "\n",
      "\n",
      "[5] CHECKPOINT STATISTICS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "[6] ADVANCED: MULTI-DEVICE & DISTRIBUTED TRAINING\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "# Cho multi-GPU training (nếu cần):\n",
      "\n",
      "# Lưu checkpoint cho distributed training\n",
      "def save_distributed_checkpoint(model, optimizer, epoch, val_loss, checkpoint_dir):\n",
      "    checkpoint = {\n",
      "        'epoch': epoch,\n",
      "        'model_state_dict': model.module.state_dict(),  # .module cho DataParallel\n",
      "        'optimizer_state_dict': optimizer.state_dict(),\n",
      "        'val_loss': val_loss,\n",
      "    }\n",
      "\n",
      "    checkpoint_path = os.path.join(checkpoint_dir, \n",
      "                                   f\"checkpoint_epoch_{epoch:03d}.pt\")\n",
      "    torch.save(checkpoint, checkpoint_path)\n",
      "    print(f\"✓ Distributed checkpoint saved: {checkpoint_path}\")\n",
      "    return checkpoint_path\n",
      "\n",
      "# Tải checkpoint cho distributed training\n",
      "def load_distributed_checkpoint(checkpoint_path, model, optimizer, device):\n",
      "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
      "\n",
      "    model.module.load_state_dict(checkpoint['model_state_dict'])  # .module\n",
      "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
      "\n",
      "    return checkpoint['epoch'], checkpoint['val_loss']\n",
      "\n",
      "\n",
      "\n",
      "[7] CLEANUP CHECKPOINTS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "[8] EXPORT MODEL TO ONNX FORMAT\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "# Export model to ONNX để deploy\n",
      "\n",
      "import torch.onnx\n",
      "\n",
      "def export_to_onnx(model, encoder_input_size, decoder_input_size, output_path):\n",
      "    '''\n",
      "    Export Seq2Seq model to ONNX format\n",
      "    '''\n",
      "    model.eval()\n",
      "\n",
      "    # Dummy inputs\n",
      "    dummy_en = torch.randint(0, 10000, (1, encoder_input_size))\n",
      "    dummy_en_len = torch.tensor([encoder_input_size])\n",
      "    dummy_de = torch.randint(0, 10000, (1, decoder_input_size))\n",
      "\n",
      "    # Export\n",
      "    torch.onnx.export(\n",
      "        model,\n",
      "        (dummy_en, dummy_en_len, dummy_de),\n",
      "        output_path,\n",
      "        opset_version=11,\n",
      "        input_names=['encoder_input', 'encoder_lengths', 'decoder_input'],\n",
      "        output_names=['output'],\n",
      "        verbose=False\n",
      "    )\n",
      "\n",
      "    print(f\"✓ Model exported to ONNX: {output_path}\")\n",
      "\n",
      "# Ví dụ: export_to_onnx(model, 30, 30, \"./model.onnx\")\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CHECKPOINT MANAGEMENT SUMMARY\n",
      "================================================================================\n",
      "\n",
      "BEST PRACTICES:\n",
      "\n",
      "1. DURING TRAINING:\n",
      "   ✓ Save checkpoint mỗi khi val_loss cải thiện (best model)\n",
      "   ✓ Save checkpoint thường xuyên (mỗi 5 epochs)\n",
      "   ✓ Giữ tối đa 5 checkpoint tốt nhất\n",
      "   ✓ Xóa checkpoint cũ để tiết kiệm storage\n",
      "\n",
      "2. TRAINING INTERRUPTION:\n",
      "   ✓ Checkpoint cho phép tiếp tục training từ điểm đó\n",
      "   ✓ Không cần re-train từ đầu\n",
      "   ✓ Tiết kiệm thời gian & resources\n",
      "\n",
      "3. INFERENCE:\n",
      "   ✓ Luôn dùng best_model.pt cho inference\n",
      "   ✓ Đảm bảo model có performance tốt nhất\n",
      "   ✓ Load model 1 lần, re-use nhiều lần\n",
      "\n",
      "4. CHECKPOINT STRUCTURE:\n",
      "   {\n",
      "     'epoch': int,\n",
      "     'model_state_dict': OrderedDict,\n",
      "     'encoder_state_dict': OrderedDict,\n",
      "     'decoder_state_dict': OrderedDict,\n",
      "     'optimizer_state_dict': OrderedDict,\n",
      "     'val_loss': float,\n",
      "     'vocab_en': list,\n",
      "     'vocab_de': list,\n",
      "     'stoi_en': dict,\n",
      "     'stoi_de': dict,\n",
      "   }\n",
      "\n",
      "5. STORAGE:\n",
      "   - Mỗi checkpoint: ~50-100MB (tùy model size)\n",
      "   - Giữ 5 checkpoints: ~250-500MB\n",
      "   - Best model riêng: ~50-100MB\n",
      "\n",
      "   Cleanup strategy:\n",
      "   ✓ Keep last 3-5 checkpoints\n",
      "   ✓ Always keep best_model.pt\n",
      "   ✓ Delete old checkpoints weekly\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECKPOINT MANAGEMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== Phần 1: Cấu hình checkpoint ==========\n",
    "\n",
    "print(\"\\n[1] CHECKPOINT CONFIGURATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"Quản lý checkpoints - lưu và tải model\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_dir=\"./checkpoints\", max_keep=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            checkpoint_dir: Thư mục lưu checkpoints\n",
    "            max_keep: Số lượng checkpoint tốt nhất cần giữ lại\n",
    "        \"\"\"\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.max_keep = max_keep\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        self.checkpoint_list = []  # [(path, val_loss), ...]\n",
    "    \n",
    "    def save_checkpoint(self, model, optimizer, epoch, val_loss, is_best=False):\n",
    "        \"\"\"\n",
    "        Lưu checkpoint\n",
    "        \n",
    "        Args:\n",
    "            model: Seq2Seq model\n",
    "            optimizer: Adam optimizer\n",
    "            epoch: Epoch hiện tại\n",
    "            val_loss: Validation loss\n",
    "            is_best: Có phải checkpoint tốt nhất không\n",
    "        \"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'encoder_state_dict': model.encoder.state_dict(),\n",
    "            'decoder_state_dict': model.decoder.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'vocab_en': vocab_en,\n",
    "            'vocab_de': vocab_de,\n",
    "            'stoi_en': stoi_en,\n",
    "            'stoi_de': stoi_de,\n",
    "        }\n",
    "        \n",
    "        # Tên file checkpoint\n",
    "        checkpoint_name = f\"checkpoint_epoch_{epoch:03d}_loss_{val_loss:.4f}.pt\"\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_name)\n",
    "        \n",
    "        # Lưu checkpoint\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"✓ Checkpoint saved: {checkpoint_path}\")\n",
    "        \n",
    "        # Lưu best model\n",
    "        if is_best:\n",
    "            best_path = os.path.join(self.checkpoint_dir, \"best_model.pt\")\n",
    "            torch.save(checkpoint, best_path)\n",
    "            print(f\"✓ Best model saved: {best_path}\")\n",
    "        \n",
    "        # Cập nhật danh sách checkpoint\n",
    "        self.checkpoint_list.append((checkpoint_path, val_loss))\n",
    "        self.checkpoint_list.sort(key=lambda x: x[1])  # Sắp xếp theo loss\n",
    "        \n",
    "        # Xóa checkpoint cũ nếu vượt quá max_keep\n",
    "        if len(self.checkpoint_list) > self.max_keep:\n",
    "            old_checkpoint = self.checkpoint_list.pop()\n",
    "            if os.path.exists(old_checkpoint[0]):\n",
    "                os.remove(old_checkpoint[0])\n",
    "                print(f\"✓ Removed old checkpoint: {old_checkpoint[0]}\")\n",
    "        \n",
    "        return checkpoint_path\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path, model, optimizer, device):\n",
    "        \"\"\"\n",
    "        Tải checkpoint\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_path: Đường dẫn file checkpoint\n",
    "            model: Seq2Seq model\n",
    "            optimizer: Adam optimizer\n",
    "            device: torch device\n",
    "        \n",
    "        Returns:\n",
    "            epoch, val_loss\n",
    "        \"\"\"\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            print(f\"✗ Checkpoint not found: {checkpoint_path}\")\n",
    "            return None, None\n",
    "        \n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        epoch = checkpoint['epoch']\n",
    "        val_loss = checkpoint['val_loss']\n",
    "        \n",
    "        print(f\"✓ Checkpoint loaded: {checkpoint_path}\")\n",
    "        print(f\"  Epoch: {epoch}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        return epoch, val_loss\n",
    "    \n",
    "    def load_best_model(self, model, device):\n",
    "        \"\"\"Tải best model\"\"\"\n",
    "        best_path = os.path.join(self.checkpoint_dir, \"best_model.pth\")\n",
    "        if not os.path.exists(best_path):\n",
    "            print(f\"✗ Best model not found: {best_path}\")\n",
    "            return\n",
    "        \n",
    "        checkpoint = torch.load(best_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"✓ Best model loaded: {best_path}\")\n",
    "        print(f\"  Val Loss: {checkpoint['val_loss']:.4f} (Epoch {checkpoint['epoch']})\")\n",
    "    \n",
    "    def list_checkpoints(self):\n",
    "        \"\"\"Liệt kê tất cả checkpoints\"\"\"\n",
    "        print(f\"\\nAvailable checkpoints in {self.checkpoint_dir}:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        checkpoint_files = sorted([\n",
    "            f for f in os.listdir(self.checkpoint_dir) \n",
    "            if f.startswith('checkpoint_') and f.endswith('.pt')\n",
    "        ])\n",
    "        \n",
    "        if not checkpoint_files:\n",
    "            print(\"No checkpoints found\")\n",
    "            return\n",
    "        \n",
    "        for i, f in enumerate(checkpoint_files, 1):\n",
    "            path = os.path.join(self.checkpoint_dir, f)\n",
    "            checkpoint = torch.load(path, map_location='cpu')\n",
    "            epoch = checkpoint['epoch']\n",
    "            val_loss = checkpoint['val_loss']\n",
    "            \n",
    "            # File size\n",
    "            size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "            \n",
    "            print(f\"{i}. {f}\")\n",
    "            print(f\"   Epoch: {epoch}, Val Loss: {val_loss:.4f}, Size: {size_mb:.1f}MB\")\n",
    "        \n",
    "        # Best model\n",
    "        best_path = os.path.join(self.checkpoint_dir, \"best_model.pt\")\n",
    "        if os.path.exists(best_path):\n",
    "            checkpoint = torch.load(best_path, map_location='cpu')\n",
    "            print(f\"\\n★ BEST MODEL: best_model.pt\")\n",
    "            print(f\"   Epoch: {checkpoint['epoch']}, Val Loss: {checkpoint['val_loss']:.4f}\")\n",
    "\n",
    "\n",
    "# Khởi tạo checkpoint manager\n",
    "checkpoint_manager = CheckpointManager(checkpoint_dir=CHECKPOINT_DIR, max_keep=5)\n",
    "print(\"✓ CheckpointManager initialized\")\n",
    "\n",
    "\n",
    "# ========== Phần 2: Cập nhật training loop với checkpoint ==========\n",
    "\n",
    "print(\"\\n\\n[2] UPDATED TRAINING LOOP WITH CHECKPOINT\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "# Thêm vào training loop (phần 8):\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for src, src_lengths, trg, trg_lengths in train_loader:\n",
    "        src = src.to(device)\n",
    "        src_lengths = src_lengths.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(src, src_lengths, trg)\n",
    "        vocab_size = outputs.size(-1)\n",
    "\n",
    "        pred = outputs[:, 1:, :].contiguous().view(-1, vocab_size)\n",
    "        target = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_train_loss = train_loss / (n_batches if n_batches > 0 else 1)\n",
    "    avg_val_loss = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "\n",
    "    if USE_SCHEDULER:\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "    # ========== CHECKPOINT SAVING ==========\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_note = \" (best -> saved)\"\n",
    "        \n",
    "        # LƯU CHECKPOINT\n",
    "        checkpoint_manager.save_checkpoint(\n",
    "            model, optimizer, epoch, avg_val_loss, is_best=True\n",
    "        )\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        best_note = \"\"\n",
    "        \n",
    "        # LƯU CHECKPOINT THƯỜNG XUYÊN (mỗi 5 epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            checkpoint_manager.save_checkpoint(\n",
    "                model, optimizer, epoch, avg_val_loss, is_best=False\n",
    "            )\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Epoch {epoch:02d} | Train loss: {avg_train_loss:.4f} | Val loss: {avg_val_loss:.4f}{best_note} | Time: {elapsed:.1f}s\")\n",
    "\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f\"Early stopping triggered. No improvement for {PATIENCE} epochs.\")\n",
    "        break\n",
    "\n",
    "print(f\"Training finished. Best val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Liệt kê tất cả checkpoints\n",
    "checkpoint_manager.list_checkpoints()\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ========== Phần 3: Khôi phục từ checkpoint ==========\n",
    "\n",
    "print(\"\\n\\n[3] RESUME TRAINING FROM CHECKPOINT\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "# Ví dụ: Khôi phục training từ checkpoint\n",
    "\n",
    "# Bước 1: Tạo model mới\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(\n",
    "    vocab_size=len(vocab_en),\n",
    "    embed_dim=512,\n",
    "    hidden_size=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    vocab_size=len(vocab_de),\n",
    "    embed_dim=512,\n",
    "    hidden_size=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Bước 2: Tải checkpoint\n",
    "checkpoint_path = \"./checkpoints/checkpoint_epoch_010_loss_3.2456.pt\"\n",
    "start_epoch, best_val_loss = checkpoint_manager.load_checkpoint(\n",
    "    checkpoint_path, model, optimizer, device\n",
    ")\n",
    "\n",
    "# Bước 3: Tiếp tục training từ epoch sau đó\n",
    "for epoch in range(start_epoch + 1, NUM_EPOCHS + 1):\n",
    "    # ... training code ...\n",
    "    pass\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ========== Phần 4: Inference từ checkpoint ==========\n",
    "\n",
    "print(\"\\n\\n[4] INFERENCE FROM CHECKPOINT\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "# Sử dụng best model để inference\n",
    "\n",
    "# Tải best model\n",
    "model_inference = Seq2Seq(encoder, decoder, device).to(device)\n",
    "checkpoint_manager.load_best_model(model_inference, device)\n",
    "\n",
    "# Dịch câu\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What is your name?\",\n",
    "    \"The weather is nice today.\"\n",
    "]\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"INFERENCE USING BEST MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for en_sent in test_sentences:\n",
    "    de_sent = translate(en_sent, model_inference, device, \n",
    "                       tokenizer_en, stoi_en, itos_de, stoi_de)\n",
    "    print(f\"EN: {en_sent}\")\n",
    "    print(f\"DE: {de_sent}\")\n",
    "    print()\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ========== Phần 5: Checkpoint Statistics ==========\n",
    "\n",
    "print(\"\\n\\n[5] CHECKPOINT STATISTICS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def analyze_checkpoints(checkpoint_dir):\n",
    "    \"\"\"Phân tích thống kê checkpoints\"\"\"\n",
    "    print(f\"\\nAnalyzing checkpoints in {checkpoint_dir}...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    checkpoint_files = [\n",
    "        f for f in os.listdir(checkpoint_dir) \n",
    "        if f.startswith('checkpoint_') and f.endswith('.pt')\n",
    "    ]\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        print(\"No checkpoints found\")\n",
    "        return\n",
    "    \n",
    "    losses = []\n",
    "    epochs = []\n",
    "    sizes = []\n",
    "    \n",
    "    for f in checkpoint_files:\n",
    "        path = os.path.join(checkpoint_dir, f)\n",
    "        checkpoint = torch.load(path, map_location='cpu')\n",
    "        \n",
    "        losses.append(checkpoint['val_loss'])\n",
    "        epochs.append(checkpoint['epoch'])\n",
    "        sizes.append(os.path.getsize(path) / (1024 * 1024))\n",
    "    \n",
    "    print(f\"Total checkpoints: {len(checkpoint_files)}\")\n",
    "    print(f\"\\nValidation Loss Statistics:\")\n",
    "    print(f\"  Min:    {min(losses):.4f}\")\n",
    "    print(f\"  Max:    {max(losses):.4f}\")\n",
    "    print(f\"  Mean:   {np.mean(losses):.4f}\")\n",
    "    print(f\"  Median: {np.median(losses):.4f}\")\n",
    "    \n",
    "    print(f\"\\nEpoch Range:\")\n",
    "    print(f\"  Min: {min(epochs)}, Max: {max(epochs)}\")\n",
    "    \n",
    "    print(f\"\\nCheckpoint File Sizes:\")\n",
    "    print(f\"  Min:  {min(sizes):.1f}MB\")\n",
    "    print(f\"  Max:  {max(sizes):.1f}MB\")\n",
    "    print(f\"  Mean: {np.mean(sizes):.1f}MB\")\n",
    "    print(f\"  Total: {sum(sizes):.1f}MB\")\n",
    "\n",
    "\n",
    "# Ví dụ: analyze_checkpoints(CHECKPOINT_DIR)\n",
    "\n",
    "\n",
    "# ========== Phần 6: Advanced - Multi-device checkpoint ==========\n",
    "\n",
    "print(\"\\n\\n[6] ADVANCED: MULTI-DEVICE & DISTRIBUTED TRAINING\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "# Cho multi-GPU training (nếu cần):\n",
    "\n",
    "# Lưu checkpoint cho distributed training\n",
    "def save_distributed_checkpoint(model, optimizer, epoch, val_loss, checkpoint_dir):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.module.state_dict(),  # .module cho DataParallel\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_loss': val_loss,\n",
    "    }\n",
    "    \n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \n",
    "                                   f\"checkpoint_epoch_{epoch:03d}.pt\")\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"✓ Distributed checkpoint saved: {checkpoint_path}\")\n",
    "    return checkpoint_path\n",
    "\n",
    "# Tải checkpoint cho distributed training\n",
    "def load_distributed_checkpoint(checkpoint_path, model, optimizer, device):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model.module.load_state_dict(checkpoint['model_state_dict'])  # .module\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    return checkpoint['epoch'], checkpoint['val_loss']\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ========== Phần 7: Cleanup function ==========\n",
    "\n",
    "print(\"\\n\\n[7] CLEANUP CHECKPOINTS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def cleanup_checkpoints(checkpoint_dir, keep_best=True, keep_last_n=3):\n",
    "    \"\"\"\n",
    "    Xóa các checkpoint cũ để tiết kiệm không gian\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_dir: Thư mục chứa checkpoints\n",
    "        keep_best: Có giữ lại best_model.pt không\n",
    "        keep_last_n: Số checkpoint gần nhất cần giữ\n",
    "    \"\"\"\n",
    "    print(f\"\\nCleaning up checkpoints in {checkpoint_dir}...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    checkpoint_files = sorted([\n",
    "        (f, os.path.getmtime(os.path.join(checkpoint_dir, f)))\n",
    "        for f in os.listdir(checkpoint_dir) \n",
    "        if f.startswith('checkpoint_') and f.endswith('.pt')\n",
    "    ], key=lambda x: x[1], reverse=True)  # Sắp xếp theo thời gian\n",
    "    \n",
    "    removed_count = 0\n",
    "    removed_size = 0\n",
    "    \n",
    "    # Giữ lại keep_last_n checkpoints gần nhất\n",
    "    for f, _ in checkpoint_files[keep_last_n:]:\n",
    "        path = os.path.join(checkpoint_dir, f)\n",
    "        size = os.path.getsize(path) / (1024 * 1024)\n",
    "        os.remove(path)\n",
    "        removed_count += 1\n",
    "        removed_size += size\n",
    "        print(f\"✓ Removed: {f} ({size:.1f}MB)\")\n",
    "    \n",
    "    print(f\"\\nRemoved {removed_count} checkpoints, freed {removed_size:.1f}MB\")\n",
    "    \n",
    "    if keep_best and os.path.exists(os.path.join(checkpoint_dir, \"best_model.pt\")):\n",
    "        print(f\"✓ Kept: best_model.pt\")\n",
    "\n",
    "\n",
    "# Ví dụ: cleanup_checkpoints(CHECKPOINT_DIR, keep_best=True, keep_last_n=3)\n",
    "\n",
    "\n",
    "# ========== Phần 8: Export to ONNX (Optional) ==========\n",
    "\n",
    "print(\"\\n\\n[8] EXPORT MODEL TO ONNX FORMAT\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "# Export model to ONNX để deploy\n",
    "\n",
    "import torch.onnx\n",
    "\n",
    "def export_to_onnx(model, encoder_input_size, decoder_input_size, output_path):\n",
    "    '''\n",
    "    Export Seq2Seq model to ONNX format\n",
    "    '''\n",
    "    model.eval()\n",
    "    \n",
    "    # Dummy inputs\n",
    "    dummy_en = torch.randint(0, 10000, (1, encoder_input_size))\n",
    "    dummy_en_len = torch.tensor([encoder_input_size])\n",
    "    dummy_de = torch.randint(0, 10000, (1, decoder_input_size))\n",
    "    \n",
    "    # Export\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (dummy_en, dummy_en_len, dummy_de),\n",
    "        output_path,\n",
    "        opset_version=11,\n",
    "        input_names=['encoder_input', 'encoder_lengths', 'decoder_input'],\n",
    "        output_names=['output'],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Model exported to ONNX: {output_path}\")\n",
    "\n",
    "# Ví dụ: export_to_onnx(model, 30, 30, \"./model.onnx\")\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ========== Phần 9: Summary ==========\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"CHECKPOINT MANAGEMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "BEST PRACTICES:\n",
    "\n",
    "1. DURING TRAINING:\n",
    "   ✓ Save checkpoint mỗi khi val_loss cải thiện (best model)\n",
    "   ✓ Save checkpoint thường xuyên (mỗi 5 epochs)\n",
    "   ✓ Giữ tối đa 5 checkpoint tốt nhất\n",
    "   ✓ Xóa checkpoint cũ để tiết kiệm storage\n",
    "\n",
    "2. TRAINING INTERRUPTION:\n",
    "   ✓ Checkpoint cho phép tiếp tục training từ điểm đó\n",
    "   ✓ Không cần re-train từ đầu\n",
    "   ✓ Tiết kiệm thời gian & resources\n",
    "\n",
    "3. INFERENCE:\n",
    "   ✓ Luôn dùng best_model.pt cho inference\n",
    "   ✓ Đảm bảo model có performance tốt nhất\n",
    "   ✓ Load model 1 lần, re-use nhiều lần\n",
    "\n",
    "4. CHECKPOINT STRUCTURE:\n",
    "   {\n",
    "     'epoch': int,\n",
    "     'model_state_dict': OrderedDict,\n",
    "     'encoder_state_dict': OrderedDict,\n",
    "     'decoder_state_dict': OrderedDict,\n",
    "     'optimizer_state_dict': OrderedDict,\n",
    "     'val_loss': float,\n",
    "     'vocab_en': list,\n",
    "     'vocab_de': list,\n",
    "     'stoi_en': dict,\n",
    "     'stoi_de': dict,\n",
    "   }\n",
    "\n",
    "5. STORAGE:\n",
    "   - Mỗi checkpoint: ~50-100MB (tùy model size)\n",
    "   - Giữ 5 checkpoints: ~250-500MB\n",
    "   - Best model riêng: ~50-100MB\n",
    "   \n",
    "   Cleanup strategy:\n",
    "   ✓ Keep last 3-5 checkpoints\n",
    "   ✓ Always keep best_model.pt\n",
    "   ✓ Delete old checkpoints weekly\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "923848e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Best model loaded and ready for inference.\n"
     ]
    }
   ],
   "source": [
    "# lado lại best moel\n",
    "model = Seq2Seq(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    device=device\n",
    ")\n",
    "model.to(device)\n",
    "state_dict = torch.load(\"best_model.pth\", map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "print(\"✓ Best model loaded and ready for inference.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
