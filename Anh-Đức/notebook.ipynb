{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38298a54",
   "metadata": {},
   "source": [
    "# 1. Chuẩn bị dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4359fd",
   "metadata": {},
   "source": [
    "# Cài đặt\n",
    "- pip install spacy\n",
    "- python -m spacy download en_core_web_sm\n",
    "- python -m spacy download de_core_news_sm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6936951",
   "metadata": {},
   "source": [
    "# 2. Tokenization – dùng Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72dab85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# English tokenizer\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "def tokenizer_en(text):\n",
    "    return spacy_en.tokenizer(text)\n",
    "\n",
    "# German tokenizer\n",
    "spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "def tokenizer_de(text):\n",
    "    return spacy_de.tokenizer(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d827fb35",
   "metadata": {},
   "source": [
    "# 3.Load EN–DE từ file .gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1a35410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def load_parallel_corpus(en_file, de_file):\n",
    "    sentences_en = []\n",
    "    sentences_de = []\n",
    "\n",
    "    with gzip.open(en_file, 'rt', encoding='utf-8') as f_en, \\\n",
    "         gzip.open(de_file, 'rt', encoding='utf-8') as f_de:\n",
    "\n",
    "        for en_line, de_line in zip(f_en, f_de):\n",
    "            en = en_line.strip()\n",
    "            de = de_line.strip()\n",
    "            sentences_en.append(en)\n",
    "            sentences_de.append(de)\n",
    "\n",
    "    return sentences_en, sentences_de\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718dac22",
   "metadata": {},
   "source": [
    "# 3.1 Load train / val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1d05515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two young, White males are outside near many bushes.\n",
      "Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n"
     ]
    }
   ],
   "source": [
    "train_en, train_de = load_parallel_corpus(\"train.en.gz\", \"train.de.gz\")\n",
    "val_en, val_de = load_parallel_corpus(\"val.en.gz\", \"val.de.gz\")\n",
    "# Kiểm tra đã load được chưa\n",
    "print(train_en[0])\n",
    "print(train_de[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3550faaf",
   "metadata": {},
   "source": [
    "# 4. Xây dựng từ điển(Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6786cd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "special_tokens = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "\n",
    "def build_vocab(sentences, tokenizer, max_words=10000):\n",
    "    counter = Counter()\n",
    "    for sent in sentences:\n",
    "        tokens = [t.text.lower() for t in tokenizer(sent)]\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # Chọn 10000 từ phổ biến nhất\n",
    "    most_common = counter.most_common(max_words - len(special_tokens))\n",
    "\n",
    "    vocab = special_tokens + [w for w, _ in most_common]\n",
    "    stoi = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "    return vocab, stoi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361cf2d6",
   "metadata": {},
   "source": [
    "# 4.1 Build vocab EN & DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7cda0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary EN: 9797\n",
      "Vocabulary DE: 10000\n"
     ]
    }
   ],
   "source": [
    "vocab_en, stoi_en = build_vocab(train_en, tokenizer_en)\n",
    "vocab_de, stoi_de = build_vocab(train_de, tokenizer_de)\n",
    "\n",
    "print(\"Vocabulary EN:\", len(vocab_en))\n",
    "print(\"Vocabulary DE:\", len(vocab_de))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6db1e85",
   "metadata": {},
   "source": [
    "# 5. Hàm convert câu → id + thêm <sos> <eos>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcb4c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(sentence, tokenizer, stoi):\n",
    "    tokens = [\"<sos>\"] + [t.text.lower() for t in tokenizer(sentence)] + [\"<eos>\"]\n",
    "    return [stoi.get(tok, stoi[\"<unk>\"]) for tok in tokens]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf9b38",
   "metadata": {},
   "source": [
    "# 5.1 Tạo dataset dạng list of (tensor_en, tensor_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d64b4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def make_dataset(en_sentences, de_sentences, tokenizer_en, tokenizer_de, stoi_en, stoi_de):\n",
    "    data = []\n",
    "    for en, de in zip(en_sentences, de_sentences):\n",
    "        en_ids = torch.tensor(numericalize(en, tokenizer_en, stoi_en))\n",
    "        de_ids = torch.tensor(numericalize(de, tokenizer_de, stoi_de))\n",
    "        data.append((en_ids, de_ids))\n",
    "    return data\n",
    "\n",
    "train_dataset = make_dataset(train_en, train_de, tokenizer_en, tokenizer_de, stoi_en, stoi_de)\n",
    "val_dataset   = make_dataset(val_en, val_de, tokenizer_en, tokenizer_de, stoi_en, stoi_de)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8f1a17",
   "metadata": {},
   "source": [
    "# 5.2 collate_fn (chuẩn cho padding + packing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3bda5fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "PAD_IDX_EN = stoi_en[\"<pad>\"]\n",
    "PAD_IDX_DE = stoi_de[\"<pad>\"]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch = [(en_ids, de_ids), ...]\n",
    "    en_list = [item[0] for item in batch]\n",
    "    de_list = [item[1] for item in batch]\n",
    "\n",
    "    # Lấy độ dài gốc\n",
    "    en_lengths = torch.tensor([len(x) for x in en_list])\n",
    "    de_lengths = torch.tensor([len(x) for x in de_list])\n",
    "\n",
    "    # Sắp xếp theo độ dài giảm dần (required for pack_padded_sequence)\n",
    "    en_lengths, sort_idx = en_lengths.sort(descending=True)\n",
    "    en_list = [en_list[i] for i in sort_idx]\n",
    "    de_list = [de_list[i] for i in sort_idx]\n",
    "    de_lengths = de_lengths[sort_idx]\n",
    "\n",
    "    # Padding\n",
    "    en_padded = pad_sequence(en_list, batch_first=True, padding_value=PAD_IDX_EN)\n",
    "    de_padded = pad_sequence(de_list, batch_first=True, padding_value=PAD_IDX_DE)\n",
    "\n",
    "    return en_padded, en_lengths, de_padded, de_lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21053afa",
   "metadata": {},
   "source": [
    "# 6. DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d9a3e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb14bc28",
   "metadata": {},
   "source": [
    "Cách dùng trong LSTM Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a81c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "def forward(self, src, src_lengths):\n",
    "    # src shape: (batch, seq_len)\n",
    "    embedded = self.embedding(src)\n",
    "\n",
    "    packed = pack_padded_sequence(\n",
    "        embedded,\n",
    "        src_lengths.cpu(),\n",
    "        batch_first=True,\n",
    "        enforce_sorted=True\n",
    "    )\n",
    "\n",
    "    outputs, hidden = self.lstm(packed)\n",
    "\n",
    "    outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
    "\n",
    "    return outputs, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a956d4f",
   "metadata": {},
   "source": [
    "# 7. Xây dựng mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b00698",
   "metadata": {},
   "source": [
    "## 7.1 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14bd0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, hidden_size=512, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=stoi_en[\"<pad>\"])\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, src, src_lengths):\n",
    "        # src: (batch, seq_len)\n",
    "        embedded = self.embedding(src)  # (B, L, E)\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=True\n",
    "        )\n",
    "\n",
    "        outputs, (h_n, c_n) = self.lstm(packed)\n",
    "\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "\n",
    "        return outputs, (h_n, c_n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a03876",
   "metadata": {},
   "source": [
    "## 7.2 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ca0f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, hidden_size=512, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=stoi_de[\"<pad>\"])\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_token, hidden):\n",
    "        # input_token: (batch,) 1 token tại bước t\n",
    "        # hidden = (h, c)\n",
    "\n",
    "        embedded = self.embedding(input_token).unsqueeze(1)  # (B,1,E)\n",
    "\n",
    "        output, hidden = self.lstm(embedded, hidden)  # output: (B,1,H)\n",
    "\n",
    "        logits = self.fc(output.squeeze(1))  # (B, vocab)\n",
    "\n",
    "        return logits, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33bcba7",
   "metadata": {},
   "source": [
    "## 7.3 Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6c33fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, teacher_forcing_ratio=0.5):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "    def forward(self, src, src_lengths, trg):\n",
    "        # src: (B, Ls)\n",
    "        # trg: (B, Lt)\n",
    "        batch_size, trg_len = trg.size()\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n",
    "\n",
    "        # ---- Encoder ----\n",
    "        _, hidden = self.encoder(src, src_lengths)\n",
    "\n",
    "        # token đầu tiên cho decoder = <sos>\n",
    "        input_token = trg[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            logits, hidden = self.decoder(input_token, hidden)\n",
    "            outputs[:, t] = logits\n",
    "\n",
    "            # chọn token dự đoán\n",
    "            predicted = logits.argmax(dim=1)\n",
    "\n",
    "            # teacher forcing ?\n",
    "            if random.random() < self.teacher_forcing_ratio:\n",
    "                input_token = trg[:, t]     # dùng ground truth\n",
    "            else:\n",
    "                input_token = predicted     # dùng dự đoán\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9a0030",
   "metadata": {},
   "source": [
    "## 7.4 Khởi tạo mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e51fe86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(\n",
    "    vocab_size=len(vocab_en),\n",
    "    embed_dim=512,\n",
    "    hidden_size=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    vocab_size=len(vocab_de),\n",
    "    embed_dim=512,\n",
    "    hidden_size=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44822159",
   "metadata": {},
   "source": [
    "# 8. Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d66af42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train loss: 3.1716 | Val loss: 3.7108 (best -> saved) | Time: 1261.8s\n",
      "Epoch 02 | Train loss: 2.8115 | Val loss: 3.6060 (best -> saved) | Time: 1360.9s\n",
      "Epoch 03 | Train loss: 2.4953 | Val loss: 3.5828 (best -> saved) | Time: 1210.6s\n",
      "Epoch 04 | Train loss: 2.2616 | Val loss: 3.6161 | Time: 1089.8s\n",
      "Epoch 05 | Train loss: 2.0136 | Val loss: 3.5954 | Time: 1060.6s\n",
      "Epoch 06 | Train loss: 1.7007 | Val loss: 3.6314 | Time: 1070.0s\n",
      "Early stopping triggered. No improvement for 3 epochs.\n",
      "Training finished. Best val loss: 3.5828\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Config\n",
    "LR = 0.001\n",
    "NUM_EPOCHS = 10       # bạn có thể đặt 10-20\n",
    "PATIENCE = 3            # early stopping nếu val_loss không giảm sau 3 epoch\n",
    "CLIP = 1.0              # grad clipping\n",
    "USE_SCHEDULER = True    # nếu muốn dùng ReduceLROnPlateau\n",
    "\n",
    "# Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX_DE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Optional scheduler\n",
    "try:\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=1, verbose=True\n",
    "    )\n",
    "except TypeError:\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=1\n",
    "    )\n",
    "# Helper: evaluation on validation set (no teacher forcing)\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    # Turn off teacher forcing during validation (full autoregressive)\n",
    "    prev_tf = getattr(model, \"teacher_forcing_ratio\", 0.0)\n",
    "    model.teacher_forcing_ratio = 0.0\n",
    "\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for src, src_lengths, trg, trg_lengths in val_loader:\n",
    "            src = src.to(device)\n",
    "            src_lengths = src_lengths.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            outputs = model(src, src_lengths, trg)  # (B, T, V)\n",
    "            vocab_size = outputs.size(-1)\n",
    "\n",
    "            # ignore the first token (<sos>) when computing loss\n",
    "            pred = outputs[:, 1:, :].contiguous().view(-1, vocab_size)   # (B*(T-1), V)\n",
    "            target = trg[:, 1:].contiguous().view(-1)                    # (B*(T-1))\n",
    "\n",
    "            loss = criterion(pred, target)\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "    model.teacher_forcing_ratio = prev_tf\n",
    "    return total_loss / (n_batches if n_batches > 0 else 1)\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for src, src_lengths, trg, trg_lengths in train_loader:\n",
    "        src = src.to(device)\n",
    "        src_lengths = src_lengths.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(src, src_lengths, trg)  # (B, T, V)\n",
    "        vocab_size = outputs.size(-1)\n",
    "\n",
    "        # shift: ignore <sos> token in loss\n",
    "        pred = outputs[:, 1:, :].contiguous().view(-1, vocab_size)  # (B*(T-1), V)\n",
    "        target = trg[:, 1:].contiguous().view(-1)                   # (B*(T-1))\n",
    "\n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_train_loss = train_loss / (n_batches if n_batches > 0 else 1)\n",
    "    avg_val_loss = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "\n",
    "    # Scheduler step on validation loss\n",
    "    if USE_SCHEDULER:\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        epochs_no_improve = 0\n",
    "        best_note = \" (best -> saved)\"\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        best_note = \"\"\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Epoch {epoch:02d} | Train loss: {avg_train_loss:.4f} | Val loss: {avg_val_loss:.4f}{best_note} | Time: {elapsed:.1f}s\")\n",
    "\n",
    "    # Early stopping\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f\"Early stopping triggered. No improvement for {PATIENCE} epochs.\")\n",
    "        break\n",
    "\n",
    "print(\"Training finished. Best val loss: {:.4f}\".format(best_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3d712",
   "metadata": {},
   "source": [
    "# 9. Dự đoán (Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4bff483b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INFERENCE EXAMPLES (Greedy Decoding)\n",
      "============================================================\n",
      "EN: Hello, how are you?\n",
      "DE: <unk> <unk> feiern.\n",
      "\n",
      "EN: What is your name?\n",
      "DE: ein künstler trägt eine <unk>.\n",
      "\n",
      "EN: The weather is nice today.\n",
      "DE: das <unk> ist eine <unk>.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Helper: Build reverse vocab (id -> token)\n",
    "def build_itos(vocab):\n",
    "    \"\"\"Index to string mapping\"\"\"\n",
    "    return {i: w for i, w in enumerate(vocab)}\n",
    "\n",
    "itos_de = build_itos(vocab_de)\n",
    "\n",
    "# Helper: Detokenize German sentence\n",
    "def detokenize_de(tokens):\n",
    "    \"\"\"\n",
    "    Ghép tokens lại thành câu (detokenize)\n",
    "    Đơn giản: join với space, sau đó xử lý dấu câu và contractions\n",
    "    \"\"\"\n",
    "    text = \" \".join(tokens)\n",
    "    # Xóa space trước dấu câu\n",
    "    text = text.replace(\" .\", \".\").replace(\" ,\", \",\").replace(\" !\", \"!\").replace(\" ?\", \"?\")\n",
    "    return text.strip()\n",
    "\n",
    "def translate(sentence: str, model, device, tokenizer_en, stoi_en, itos_de, stoi_de, \n",
    "              max_length=50, beam_width=1) -> str:\n",
    "    \"\"\"\n",
    "    Dịch câu tiếng Anh sang tiếng Đức (Greedy Decoding).\n",
    "    \n",
    "    Args:\n",
    "        sentence: Input English sentence\n",
    "        model: Seq2Seq model\n",
    "        device: torch device (cpu/cuda)\n",
    "        tokenizer_en: Spacy English tokenizer\n",
    "        stoi_en: English string-to-index vocab\n",
    "        itos_de: German index-to-string vocab\n",
    "        max_length: Maximum output length\n",
    "        beam_width: 1 for greedy, >1 for beam search (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Translated German sentence as string\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # ---- 1. Tokenize + Numericalize input (English) ----\n",
    "    tokens_en = [t.text.lower() for t in tokenizer_en(sentence)]\n",
    "    input_ids = [stoi_en.get(\"<sos>\", 1)] + [stoi_en.get(tok, stoi_en[\"<unk>\"]) for tok in tokens_en] + [stoi_en.get(\"<eos>\", 3)]\n",
    "    src_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)  # (1, seq_len)\n",
    "    src_length = torch.tensor([len(input_ids)], dtype=torch.long).to(device)  # (1,)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # ---- 2. Encoder ----\n",
    "        _, hidden = model.encoder(src_tensor, src_length)\n",
    "        \n",
    "        # ---- 3. Decoder (Greedy) ----\n",
    "        output_ids = [stoi_de[\"<sos>\"]]  # Start with <sos>\n",
    "        input_token = torch.tensor([stoi_de[\"<sos>\"]], dtype=torch.long).to(device)  # (1,)\n",
    "        \n",
    "        for t in range(1, max_length):\n",
    "            logits, hidden = model.decoder(input_token, hidden)  # (1, vocab_size)\n",
    "            \n",
    "            # Greedy: chọn token có xác suất cao nhất\n",
    "            predicted_id = logits.argmax(dim=1).item()  # scalar\n",
    "            output_ids.append(predicted_id)\n",
    "            \n",
    "            # Nếu gặp <eos>, dừng\n",
    "            if predicted_id == stoi_de[\"<eos>\"]:\n",
    "                break\n",
    "            \n",
    "            # input cho bước tiếp theo\n",
    "            input_token = torch.tensor([predicted_id], dtype=torch.long).to(device)\n",
    "    \n",
    "    # ---- 4. Detokenize: convert id → token → text ----\n",
    "    # Bỏ <sos> và <eos>\n",
    "    output_tokens = [itos_de.get(idx, \"<unk>\") for idx in output_ids[1:]]\n",
    "    if output_tokens and output_tokens[-1] == \"<eos>\":\n",
    "        output_tokens = output_tokens[:-1]\n",
    "    \n",
    "    translated_sentence = detokenize_de(output_tokens)\n",
    "    \n",
    "    return translated_sentence\n",
    "\n",
    "\n",
    "# ---- Test examples ----\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What is your name?\",\n",
    "    \"The weather is nice today.\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INFERENCE EXAMPLES (Greedy Decoding)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for en_sent in test_sentences:\n",
    "    de_sent = translate(en_sent, model, device, tokenizer_en, stoi_en, itos_de, stoi_de)\n",
    "    print(f\"EN: {en_sent}\")\n",
    "    print(f\"DE: {de_sent}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db6b62c",
   "metadata": {},
   "source": [
    "# 10. Đánh giá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33429955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 200 test sentences\n",
      "\n",
      "Translating test set...\n",
      "Translated 200 sentences\n",
      "\n",
      "======================================================================\n",
      "EVALUATION METRICS\n",
      "======================================================================\n",
      "\n",
      "BLEU Score (average): 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 676.4170\n",
      "Average Loss: 6.5168\n",
      "\n",
      "======================================================================\n",
      "DETAILED EXAMPLES & ERROR ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "--- TOP 5 BEST TRANSLATIONS (Highest BLEU) ---\n",
      "\n",
      "1. BLEU: 0.3156\n",
      "   EN:  A man playing a keyboard and singing into a microphone.\n",
      "   REF: Eine Frau spielt Keyboard und singt in ein Mikrofon.\n",
      "   PRED: ein mann spielt gitarre und singt in ein mikrofon.\n",
      "\n",
      "2. BLEU: 0.3156\n",
      "   EN:  A brown dog chewing on a large piece of wood.\n",
      "   REF: Ein brauner Hund kaut auf einem großen Holzstück herum.\n",
      "   PRED: ein brauner hund kaut auf einem großen stück holz.\n",
      "\n",
      "3. BLEU: 0.2790\n",
      "   EN:  A man sleeping in a green room on a couch.\n",
      "   REF: Ein Mann schläft in einem grünen Raum auf einem Sofa.\n",
      "   PRED: ein mann schläft in einem grünen zimmer auf einem grünen sofa.\n",
      "\n",
      "4. BLEU: 0.1750\n",
      "   EN:  A balding man wearing a red life jacket is sitting in a small boat.\n",
      "   REF: Ein Mann mit beginnender Glatze, der eine rote Rettungsweste trägt, sitzt in einem kleinen Boot.\n",
      "   PRED: ein mann mit einer roten jacke sitzt in einem kleinen boot auf einem kleinen.\n",
      "\n",
      "5. BLEU: 0.0000\n",
      "   EN:  They are posing for a picture.\n",
      "   REF: Sie posieren für ein Bild.\n",
      "   PRED: sie posieren für ein foto.\n",
      "\n",
      "\n",
      "--- TOP 5 WORST TRANSLATIONS (Lowest BLEU) ---\n",
      "\n",
      "1. BLEU: 0.0000\n",
      "   EN:  A cute baby is smiling at another child.\n",
      "   REF: Ein süßes Baby lächelt einem anderen Kind zu.\n",
      "   PRED: ein lächelndes baby, das ein kleines baby..\n",
      "   ERROR ANALYSIS:\n",
      "     - Missing words: Baby, Ein, zu., anderen, einem\n",
      "     - Extra words: baby.., lächelndes, ein, baby,, kleines\n",
      "\n",
      "2. BLEU: 0.0000\n",
      "   EN:  A tractor is moving dirt to help build up a retaining wall.\n",
      "   REF: Ein Traktor bewegt Erde für den Bau einer Stützmauer.\n",
      "   PRED: ein <unk> fliegt eine steile <unk> hoch, um eine <unk> <unk>.\n",
      "   ERROR ANALYSIS:\n",
      "     - Missing words: bewegt, Ein, Traktor, Bau, Erde\n",
      "     - Extra words: um, <unk>., hoch,, eine, steile\n",
      "\n",
      "3. BLEU: 0.0000\n",
      "   EN:  A janitor about to mop in a train station.\n",
      "   REF: Eine Reinigungskraft ist im Begriff, eine Bahnstation zu wischen.\n",
      "   PRED: ein hausmeister in in in einem zug.\n",
      "   ERROR ANALYSIS:\n",
      "     - Missing words: im, ist, Reinigungskraft, eine, zu\n",
      "     - Extra words: in, zug., einem, ein, hausmeister\n",
      "\n",
      "4. BLEU: 0.0000\n",
      "   EN:  Baby looking at the leaves on a branch of a tree.\n",
      "   REF: Baby sieht sich die Blätter am Zweig eines Baumes an.\n",
      "   PRED: ein baby blickt auf einen umgestürzten an einem verschneiten nachmittag.\n",
      "   ERROR ANALYSIS:\n",
      "     - Missing words: Zweig, sieht, Baby, eines, Blätter\n",
      "     - Extra words: verschneiten, baby, einen, blickt, auf\n",
      "\n",
      "5. BLEU: 0.0000\n",
      "   EN:  Twp children dig holes in the dirt.\n",
      "   REF: Zwei Kinder graben Löcher in die Erde.\n",
      "   PRED: kinder kinder kinder im im dreck.\n",
      "   ERROR ANALYSIS:\n",
      "     - Missing words: in, Kinder, Löcher, Zwei, Erde.\n",
      "     - Extra words: kinder, im, dreck.\n",
      "\n",
      "\n",
      "======================================================================\n",
      "BLEU SCORE DISTRIBUTION\n",
      "======================================================================\n",
      "\n",
      "Min BLEU:    0.0000\n",
      "Max BLEU:    0.3156\n",
      "Mean BLEU:   0.0054\n",
      "Median BLEU: 0.0000\n",
      "Std BLEU:    0.0388\n",
      "\n",
      "BLEU Score Distribution by Range:\n",
      "  0.0-0.2:  197 ( 98.5%)\n",
      "  0.2-0.4:    3 (  1.5%)\n",
      "  0.4-0.6:    0 (  0.0%)\n",
      "  0.6-0.8:    0 (  0.0%)\n",
      "  0.8-1.0:    0 (  0.0%)\n",
      "\n",
      "======================================================================\n",
      "COMMON ERROR PATTERNS\n",
      "======================================================================\n",
      "\n",
      "Error Pattern Frequencies (out of 200 sentences):\n",
      "  length_mismatch:  130 ( 65.0%)\n",
      "  word_substitution:  200 (100.0%)\n",
      "  omission:    6 (  3.0%)\n",
      "  insertion:    2 (  1.0%)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Load test set (hoặc dùng tập val nếu không có test riêng)\n",
    "# test_en, test_de = load_parallel_corpus(\"test.en.gz\", \"test.de.gz\")\n",
    "# Tạm dùng val set để demo\n",
    "test_en, test_de = val_en[:200], val_de[:200]  # Lấy 200 câu từ val set\n",
    "\n",
    "print(f\"Evaluating on {len(test_en)} test sentences\")\n",
    "\n",
    "# ========== 1. Tính BLEU Score ==========\n",
    "\n",
    "def compute_bleu_score(references, hypotheses):\n",
    "    \"\"\"\n",
    "    Tính BLEU score trung bình trên corpus\n",
    "    \n",
    "    Args:\n",
    "        references: list of list of reference sentences (tokens)\n",
    "        hypotheses: list of hypothesis sentences (tokens)\n",
    "    \n",
    "    Returns:\n",
    "        bleu_score (0-1)\n",
    "    \"\"\"\n",
    "    total_bleu = 0.0\n",
    "    n = len(hypotheses)\n",
    "    \n",
    "    bleu_scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        # sentence_bleu expects: reference (list of list), hypothesis (list)\n",
    "        ref_tokens = ref.split()\n",
    "        hyp_tokens = hyp.split()\n",
    "        \n",
    "        # weights for 1-gram, 2-gram, 3-gram, 4-gram\n",
    "        weights = (0.25, 0.25, 0.25, 0.25)\n",
    "        bleu = sentence_bleu([ref_tokens], hyp_tokens, weights=weights)\n",
    "        bleu_scores.append(bleu)\n",
    "        total_bleu += bleu\n",
    "    \n",
    "    avg_bleu = total_bleu / n\n",
    "    return avg_bleu, bleu_scores\n",
    "\n",
    "\n",
    "# ========== 2. Tính Perplexity ==========\n",
    "\n",
    "def compute_perplexity(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Tính Perplexity trên test set\n",
    "    Perplexity = exp(loss)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, src_lengths, trg, trg_lengths in test_loader:\n",
    "            src = src.to(device)\n",
    "            src_lengths = src_lengths.to(device)\n",
    "            trg = trg.to(device)\n",
    "            \n",
    "            outputs = model(src, src_lengths, trg)\n",
    "            vocab_size = outputs.size(-1)\n",
    "            \n",
    "            pred = outputs[:, 1:, :].contiguous().view(-1, vocab_size)\n",
    "            target = trg[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(pred, target)\n",
    "            total_loss += loss.item() * target.size(0)\n",
    "            n_tokens += (target != PAD_IDX_DE).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / n_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    return perplexity, avg_loss\n",
    "\n",
    "\n",
    "# ========== 3. Tạo Test DataLoader ==========\n",
    "\n",
    "test_dataset = make_dataset(test_en, test_de, tokenizer_en, tokenizer_de, stoi_en, stoi_de)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "# ========== 4. Dịch toàn bộ test set ==========\n",
    "\n",
    "print(\"\\nTranslating test set...\")\n",
    "predictions = []\n",
    "for en_sent in test_en:\n",
    "    de_pred = translate(en_sent, model, device, tokenizer_en, stoi_en, itos_de, stoi_de)\n",
    "    predictions.append(de_pred)\n",
    "\n",
    "print(f\"Translated {len(predictions)} sentences\")\n",
    "\n",
    "\n",
    "# ========== 5. Tính BLEU & Perplexity ==========\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# BLEU Score\n",
    "avg_bleu, bleu_scores = compute_bleu_score(test_de, predictions)\n",
    "print(f\"\\nBLEU Score (average): {avg_bleu:.4f}\")\n",
    "\n",
    "# Perplexity\n",
    "perplexity, avg_loss = compute_perplexity(model, test_loader, criterion, device)\n",
    "print(f\"Perplexity: {perplexity:.4f}\")\n",
    "print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "# ========== 6. Error Analysis: 5 ví dụ đúng + sai ==========\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETAILED EXAMPLES & ERROR ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sắp xếp theo BLEU score để lấy ví dụ tốt nhất và xấu nhất\n",
    "indices = np.argsort(bleu_scores)\n",
    "\n",
    "# 5 ví dụ tốt nhất (highest BLEU)\n",
    "print(\"\\n--- TOP 5 BEST TRANSLATIONS (Highest BLEU) ---\\n\")\n",
    "best_indices = indices[-5:][::-1]\n",
    "for rank, idx in enumerate(best_indices, 1):\n",
    "    en = test_en[idx]\n",
    "    de_ref = test_de[idx]\n",
    "    de_pred = predictions[idx]\n",
    "    bleu = bleu_scores[idx]\n",
    "    \n",
    "    print(f\"{rank}. BLEU: {bleu:.4f}\")\n",
    "    print(f\"   EN:  {en}\")\n",
    "    print(f\"   REF: {de_ref}\")\n",
    "    print(f\"   PRED: {de_pred}\")\n",
    "    print()\n",
    "\n",
    "# 5 ví dụ tệ nhất (lowest BLEU)\n",
    "print(\"\\n--- TOP 5 WORST TRANSLATIONS (Lowest BLEU) ---\\n\")\n",
    "worst_indices = indices[:5]\n",
    "for rank, idx in enumerate(worst_indices, 1):\n",
    "    en = test_en[idx]\n",
    "    de_ref = test_de[idx]\n",
    "    de_pred = predictions[idx]\n",
    "    bleu = bleu_scores[idx]\n",
    "    \n",
    "    print(f\"{rank}. BLEU: {bleu:.4f}\")\n",
    "    print(f\"   EN:  {en}\")\n",
    "    print(f\"   REF: {de_ref}\")\n",
    "    print(f\"   PRED: {de_pred}\")\n",
    "    \n",
    "    # Phân tích lỗi\n",
    "    ref_tokens = set(de_ref.split())\n",
    "    pred_tokens = set(de_pred.split())\n",
    "    \n",
    "    missing = ref_tokens - pred_tokens\n",
    "    extra = pred_tokens - ref_tokens\n",
    "    \n",
    "    if missing or extra:\n",
    "        print(f\"   ERROR ANALYSIS:\")\n",
    "        if missing:\n",
    "            print(f\"     - Missing words: {', '.join(list(missing)[:5])}\")\n",
    "        if extra:\n",
    "            print(f\"     - Extra words: {', '.join(list(extra)[:5])}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# ========== 7. Thống kê BLEU Distribution ==========\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BLEU SCORE DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "bleu_array = np.array(bleu_scores)\n",
    "print(f\"\\nMin BLEU:    {bleu_array.min():.4f}\")\n",
    "print(f\"Max BLEU:    {bleu_array.max():.4f}\")\n",
    "print(f\"Mean BLEU:   {bleu_array.mean():.4f}\")\n",
    "print(f\"Median BLEU: {np.median(bleu_array):.4f}\")\n",
    "print(f\"Std BLEU:    {bleu_array.std():.4f}\")\n",
    "\n",
    "# Phân loại theo BLEU ranges\n",
    "bleu_ranges = {\n",
    "    \"0.0-0.2\": (bleu_array >= 0.0) & (bleu_array < 0.2),\n",
    "    \"0.2-0.4\": (bleu_array >= 0.2) & (bleu_array < 0.4),\n",
    "    \"0.4-0.6\": (bleu_array >= 0.4) & (bleu_array < 0.6),\n",
    "    \"0.6-0.8\": (bleu_array >= 0.6) & (bleu_array < 0.8),\n",
    "    \"0.8-1.0\": (bleu_array >= 0.8) & (bleu_array <= 1.0),\n",
    "}\n",
    "\n",
    "print(\"\\nBLEU Score Distribution by Range:\")\n",
    "for range_name, mask in bleu_ranges.items():\n",
    "    count = mask.sum()\n",
    "    pct = 100 * count / len(bleu_array)\n",
    "    print(f\"  {range_name}: {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "\n",
    "# ========== 8. Common Error Patterns ==========\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMMON ERROR PATTERNS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "error_patterns = {\n",
    "    \"length_mismatch\": 0,\n",
    "    \"word_substitution\": 0,\n",
    "    \"omission\": 0,\n",
    "    \"insertion\": 0,\n",
    "}\n",
    "\n",
    "for idx in range(len(test_de)):\n",
    "    ref_tokens = test_de[idx].split()\n",
    "    pred_tokens = predictions[idx].split()\n",
    "    \n",
    "    if len(pred_tokens) < len(ref_tokens) * 0.7:\n",
    "        error_patterns[\"omission\"] += 1\n",
    "    elif len(pred_tokens) > len(ref_tokens) * 1.3:\n",
    "        error_patterns[\"insertion\"] += 1\n",
    "    elif len(pred_tokens) != len(ref_tokens):\n",
    "        error_patterns[\"length_mismatch\"] += 1\n",
    "    \n",
    "    if ref_tokens != pred_tokens:\n",
    "        # Check for word substitutions\n",
    "        matching = sum(1 for r, p in zip(ref_tokens, pred_tokens) if r == p)\n",
    "        if matching < len(ref_tokens):\n",
    "            error_patterns[\"word_substitution\"] += 1\n",
    "\n",
    "print(\"\\nError Pattern Frequencies (out of {} sentences):\".format(len(test_de)))\n",
    "for pattern, count in error_patterns.items():\n",
    "    pct = 100 * count / len(test_de)\n",
    "    print(f\"  {pattern}: {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77668fe9",
   "metadata": {},
   "source": [
    "# 11. Xử lý các phần khó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "527b1808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TROUBLESHOOTING & DEBUGGING GUIDE\n",
      "================================================================================\n",
      "\n",
      "[1] CHECKING TENSOR SHAPES\n",
      "--------------------------------------------------------------------------------\n",
      "Sample batch shapes:\n",
      "  src shape:          torch.Size([64, 32]) (batch, seq_len)\n",
      "  src_lengths shape:  torch.Size([64])\n",
      "  trg shape:          torch.Size([64, 25])\n",
      "  trg_lengths shape:  torch.Size([64])\n",
      "\n",
      "  model output shape: torch.Size([64, 25, 10000]) (batch, seq_len, vocab_size)\n",
      "  Expected: (64, 25, 10000)\n",
      "\n",
      "  pred shape (after reshape): torch.Size([1536, 10000])\n",
      "  target shape (after reshape): torch.Size([1536])\n",
      "  loss: 1.2930\n",
      "\n",
      "\n",
      "[2] CHECKING DATA NORMALIZATION\n",
      "--------------------------------------------------------------------------------\n",
      "English sentence lengths:\n",
      "  Min: 3, Max: 37, Mean: 11.9\n",
      "  Median: 11.0, Std: 3.8\n",
      "\n",
      "German sentence lengths:\n",
      "  Min: 1, Max: 39, Mean: 11.1\n",
      "  Median: 11.0, Std: 3.8\n",
      "\n",
      "Sentences longer than 50 tokens:\n",
      "  EN: 0 (0.0%)\n",
      "  DE: 0 (0.0%)\n",
      "\n",
      "\n",
      "[3] CHECKING LEARNING RATE & GRADIENTS\n",
      "--------------------------------------------------------------------------------\n",
      "Gradient Norm: 0.6340\n",
      "Learning Rate: 0.000500\n",
      "✓ Gradient norm looks reasonable\n",
      "\n",
      "\n",
      "[4] TEACHER FORCING & EXPOSURE BIAS\n",
      "--------------------------------------------------------------------------------\n",
      "Current teacher_forcing_ratio: 0.5\n",
      "\n",
      "Recommendations:\n",
      "  - Start with 0.5 (50% ground truth, 50% predictions)\n",
      "  - Use scheduled sampling: gradually decrease ratio during training\n",
      "  - Formula: tf_ratio = initial * exp(-decay * epoch)\n",
      "\n",
      "Implementation example:\n",
      "\n",
      "# Scheduled teacher forcing\n",
      "def get_tf_ratio(epoch, initial_tf=0.5, decay=0.05):\n",
      "    return initial_tf * math.exp(-decay * epoch)\n",
      "\n",
      "# In training loop:\n",
      "model.teacher_forcing_ratio = get_tf_ratio(epoch)\n",
      "\n",
      "\n",
      "\n",
      "[5] OVERFITTING DETECTION\n",
      "--------------------------------------------------------------------------------\n",
      "Training Loss (first vs last): 3.1716 → 1.7007\n",
      "Validation Loss (first vs last): 3.7108 → 3.6314\n",
      "Train-Val Gap: 1.9308\n",
      "\n",
      "⚠️  WARNING: Significant overfitting detected!\n",
      "\n",
      "Solutions:\n",
      "  1. Increase dropout (currently 0.3)\n",
      "  2. Add L2 regularization (weight decay)\n",
      "  3. Use early stopping (already enabled)\n",
      "  4. Filter long sentences (max 50 tokens)\n",
      "  5. Increase batch size\n",
      "\n",
      "\n",
      "[6] DIAGNOSING 'LOSS NOT DECREASING' ISSUES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Common causes and solutions:\n",
      "\n",
      "1. LEARNING RATE TOO HIGH\n",
      "   - Symptom: Loss oscillates or increases\n",
      "   - Solution: Reduce LR (e.g., 0.001 → 0.0005)\n",
      "\n",
      "2. LEARNING RATE TOO LOW\n",
      "   - Symptom: Loss decreases very slowly\n",
      "   - Solution: Increase LR (e.g., 0.0001 → 0.001)\n",
      "\n",
      "3. GRADIENT VANISHING/EXPLODING\n",
      "   - Symptom: Loss becomes NaN or Inf\n",
      "   - Solution: Check gradient norm, increase CLIP value, use gradient clipping\n",
      "\n",
      "4. BAD DATA\n",
      "   - Symptom: Loss plateaus at high value\n",
      "   - Solution: Check data quality, verify tokenization, ensure padding is correct\n",
      "\n",
      "5. MODEL TOO SMALL\n",
      "   - Symptom: Slow improvement on training set\n",
      "   - Solution: Increase embed_dim, hidden_size, or num_layers\n",
      "\n",
      "6. BATCH SIZE ISSUES\n",
      "   - Too small: Noisy gradients, slow training\n",
      "   - Too large: Memory issues, poor generalization\n",
      "   - Try: 32, 64, 128\n",
      "\n",
      "\n",
      "\n",
      "[7] MEMORY & PERFORMANCE OPTIMIZATION\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Memory-saving strategies:\n",
      "\n",
      "1. FILTER LONG SENTENCES\n",
      "   - Limit to max_len=50 tokens\n",
      "   - Code example:\n",
      "\n",
      "   def filter_by_length(en_sents, de_sents, max_len=50):\n",
      "       data = [(en, de) for en, de in zip(en_sents, de_sents)\n",
      "               if len(en.split()) <= max_len and len(de.split()) <= max_len]\n",
      "       en_filtered, de_filtered = zip(*data)\n",
      "       return list(en_filtered), list(de_filtered)\n",
      "\n",
      "   train_en, train_de = filter_by_length(train_en, train_de, max_len=50)\n",
      "\n",
      "2. REDUCE VOCAB SIZE\n",
      "   - Currently: 10,000 words\n",
      "   - Try: 5,000 or 8,000\n",
      "   - Trade-off: Less <unk> tokens vs smaller model\n",
      "\n",
      "3. REDUCE EMBEDDING/HIDDEN DIMENSION\n",
      "   - Current: embed_dim=512, hidden_size=512\n",
      "   - Try: 256 or 384\n",
      "   - Still gets decent results with lower memory\n",
      "\n",
      "4. USE GRADIENT ACCUMULATION (if needed)\n",
      "   - Simulate larger batch size with smaller batches\n",
      "\n",
      "5. MIXED PRECISION (if using CUDA)\n",
      "   - Use torch.cuda.amp for faster computation\n",
      "\n",
      "\n",
      "\n",
      "[8] TRAINING MONITORING CHECKLIST\n",
      "--------------------------------------------------------------------------------\n",
      "  ☐ Tensor shapes                  - ✓ Verify in [1]\n",
      "  ☐ Data stats                     - ✓ Check in [2]\n",
      "  ☐ Gradient flow                  - ✓ Monitor in [3]\n",
      "  ☐ Teacher forcing                - ✓ Review in [4]\n",
      "  ☐ Overfitting                    - ✓ Assess in [5]\n",
      "  ☐ Learning rate                  - Adjust based on loss curve\n",
      "  ☐ Loss trend                     - Should decrease monotonically (with fluctuations)\n",
      "  ☐ Validation loss                - Should decrease, gap with train loss < 0.5\n",
      "  ☐ Checkpoints                    - Save best model (already doing)\n",
      "  ☐ Early stopping                 - Patience=3 (already enabled)\n",
      "\n",
      "\n",
      "[9] QUICK DEBUG: Run this if loss gets stuck\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "# Step 1: Check a single batch\n",
      "src, src_lengths, trg, trg_lengths = next(iter(train_loader))\n",
      "print(\"Input shapes OK:\", src.shape, trg.shape)\n",
      "\n",
      "# Step 2: Forward pass\n",
      "model.eval()\n",
      "with torch.no_grad():\n",
      "    out = model(src.to(device), src_lengths.to(device), trg.to(device))\n",
      "    print(\"Output shape OK:\", out.shape)\n",
      "\n",
      "# Step 3: Compute loss manually\n",
      "pred = out[:, 1:, :].contiguous().view(-1, len(vocab_de))\n",
      "target = trg[:, 1:].contiguous().view(-1)\n",
      "loss = criterion(pred, target)\n",
      "print(\"Loss OK:\", loss.item())\n",
      "\n",
      "# Step 4: Check for NaN/Inf\n",
      "print(\"Contains NaN:\", torch.isnan(out).any().item())\n",
      "print(\"Contains Inf:\", torch.isinf(out).any().item())\n",
      "\n",
      "\n",
      "================================================================================\n",
      "END TROUBLESHOOTING GUIDE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TROUBLESHOOTING & DEBUGGING GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== 1. Kiểm tra Shape của Tensors ==========\n",
    "\n",
    "print(\"\\n[1] CHECKING TENSOR SHAPES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def check_tensor_shapes():\n",
    "    \"\"\"Kiểm tra shape của các tensor trong training\"\"\"\n",
    "    print(\"Sample batch shapes:\")\n",
    "    \n",
    "    # Lấy một batch để kiểm tra\n",
    "    for src, src_lengths, trg, trg_lengths in train_loader:\n",
    "        print(f\"  src shape:          {src.shape} (batch, seq_len)\")\n",
    "        print(f\"  src_lengths shape:  {src_lengths.shape}\")\n",
    "        print(f\"  trg shape:          {trg.shape}\")\n",
    "        print(f\"  trg_lengths shape:  {trg_lengths.shape}\")\n",
    "        \n",
    "        src = src.to(device)\n",
    "        src_lengths = src_lengths.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        # Forward pass (training mode)\n",
    "        model.train()\n",
    "        outputs = model(src, src_lengths, trg)\n",
    "        \n",
    "        print(f\"\\n  model output shape: {outputs.shape} (batch, seq_len, vocab_size)\")\n",
    "        print(f\"  Expected: ({src.size(0)}, {trg.size(1)}, {len(vocab_de)})\")\n",
    "        \n",
    "        # Kiểm tra loss\n",
    "        vocab_size = outputs.size(-1)\n",
    "        pred = outputs[:, 1:, :].contiguous().view(-1, vocab_size)\n",
    "        target = trg[:, 1:].contiguous().view(-1)\n",
    "        \n",
    "        print(f\"\\n  pred shape (after reshape): {pred.shape}\")\n",
    "        print(f\"  target shape (after reshape): {target.shape}\")\n",
    "        \n",
    "        loss = criterion(pred, target)\n",
    "        print(f\"  loss: {loss.item():.4f}\")\n",
    "        \n",
    "        break\n",
    "\n",
    "check_tensor_shapes()\n",
    "\n",
    "\n",
    "# ========== 2. Kiểm tra Data Normalization ==========\n",
    "\n",
    "print(\"\\n\\n[2] CHECKING DATA NORMALIZATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def check_data_stats():\n",
    "    \"\"\"Kiểm tra thống kê dữ liệu: độ dài câu, phân bố từ\"\"\"\n",
    "    \n",
    "    # Độ dài câu\n",
    "    en_lengths = [len(s.split()) for s in train_en]\n",
    "    de_lengths = [len(s.split()) for s in train_de]\n",
    "    \n",
    "    print(\"English sentence lengths:\")\n",
    "    print(f\"  Min: {min(en_lengths)}, Max: {max(en_lengths)}, Mean: {np.mean(en_lengths):.1f}\")\n",
    "    print(f\"  Median: {np.median(en_lengths):.1f}, Std: {np.std(en_lengths):.1f}\")\n",
    "    \n",
    "    print(\"\\nGerman sentence lengths:\")\n",
    "    print(f\"  Min: {min(de_lengths)}, Max: {max(de_lengths)}, Mean: {np.mean(de_lengths):.1f}\")\n",
    "    print(f\"  Median: {np.median(de_lengths):.1f}, Std: {np.std(de_lengths):.1f}\")\n",
    "    \n",
    "    # Cảnh báo nếu có câu quá dài\n",
    "    max_len_threshold = 50\n",
    "    en_too_long = sum(1 for l in en_lengths if l > max_len_threshold)\n",
    "    de_too_long = sum(1 for l in de_lengths if l > max_len_threshold)\n",
    "    \n",
    "    print(f\"\\nSentences longer than {max_len_threshold} tokens:\")\n",
    "    print(f\"  EN: {en_too_long} ({100*en_too_long/len(en_lengths):.1f}%)\")\n",
    "    print(f\"  DE: {de_too_long} ({100*de_too_long/len(de_lengths):.1f}%)\")\n",
    "    \n",
    "    if en_too_long > 0 or de_too_long > 0:\n",
    "        print(\"\\n  ⚠️ TIP: Consider filtering sentences > 50 tokens to reduce memory usage\")\n",
    "        print(\"         and improve training stability\")\n",
    "\n",
    "check_data_stats()\n",
    "\n",
    "\n",
    "# ========== 3. Learning Rate & Gradient Check ==========\n",
    "\n",
    "print(\"\\n\\n[3] CHECKING LEARNING RATE & GRADIENTS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def check_gradients():\n",
    "    \"\"\"Kiểm tra gradient flow\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Lấy một batch\n",
    "    for src, src_lengths, trg, trg_lengths in train_loader:\n",
    "        src = src.to(device)\n",
    "        src_lengths = src_lengths.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(src, src_lengths, trg)\n",
    "        vocab_size = outputs.size(-1)\n",
    "        \n",
    "        pred = outputs[:, 1:, :].contiguous().view(-1, vocab_size)\n",
    "        target = trg[:, 1:].contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Kiểm tra gradient norm\n",
    "        total_norm = 0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "        \n",
    "        print(f\"Gradient Norm: {total_norm:.4f}\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        if total_norm > 100:\n",
    "            print(\"⚠️  WARNING: Large gradient norm detected!\")\n",
    "            print(\"   - Consider increasing CLIP value or reducing learning rate\")\n",
    "        elif total_norm < 0.0001:\n",
    "            print(\"⚠️  WARNING: Very small gradient norm!\")\n",
    "            print(\"   - Check if loss is saturating or learning rate is too small\")\n",
    "        else:\n",
    "            print(\"✓ Gradient norm looks reasonable\")\n",
    "        \n",
    "        break\n",
    "\n",
    "check_gradients()\n",
    "\n",
    "\n",
    "# ========== 4. Teacher Forcing Analysis ==========\n",
    "\n",
    "print(\"\\n\\n[4] TEACHER FORCING & EXPOSURE BIAS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"Current teacher_forcing_ratio: {model.teacher_forcing_ratio}\")\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"  - Start with 0.5 (50% ground truth, 50% predictions)\")\n",
    "print(\"  - Use scheduled sampling: gradually decrease ratio during training\")\n",
    "print(\"  - Formula: tf_ratio = initial * exp(-decay * epoch)\")\n",
    "print(\"\\nImplementation example:\")\n",
    "print(\"\"\"\n",
    "# Scheduled teacher forcing\n",
    "def get_tf_ratio(epoch, initial_tf=0.5, decay=0.05):\n",
    "    return initial_tf * math.exp(-decay * epoch)\n",
    "\n",
    "# In training loop:\n",
    "model.teacher_forcing_ratio = get_tf_ratio(epoch)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ========== 5. Overfitting Check ==========\n",
    "\n",
    "print(\"\\n\\n[5] OVERFITTING DETECTION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if len(history[\"train_loss\"]) > 2 and len(history[\"val_loss\"]) > 2:\n",
    "    train_loss_trend = history[\"train_loss\"][-1] < history[\"train_loss\"][0]\n",
    "    val_loss_trend = history[\"val_loss\"][-1] > history[\"val_loss\"][0]\n",
    "    \n",
    "    gap = history[\"val_loss\"][-1] - history[\"train_loss\"][-1]\n",
    "    \n",
    "    print(f\"Training Loss (first vs last): {history['train_loss'][0]:.4f} → {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Validation Loss (first vs last): {history['val_loss'][0]:.4f} → {history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Train-Val Gap: {gap:.4f}\")\n",
    "    \n",
    "    if gap > 0.5:\n",
    "        print(\"\\n⚠️  WARNING: Significant overfitting detected!\")\n",
    "        print(\"\\nSolutions:\")\n",
    "        print(\"  1. Increase dropout (currently 0.3)\")\n",
    "        print(\"  2. Add L2 regularization (weight decay)\")\n",
    "        print(\"  3. Use early stopping (already enabled)\")\n",
    "        print(\"  4. Filter long sentences (max 50 tokens)\")\n",
    "        print(\"  5. Increase batch size\")\n",
    "    else:\n",
    "        print(\"\\n✓ Overfitting levels look reasonable\")\n",
    "else:\n",
    "    print(\"Not enough epochs completed yet to assess overfitting\")\n",
    "\n",
    "\n",
    "# ========== 6. Loss Not Decreasing - Diagnostic ==========\n",
    "\n",
    "print(\"\\n\\n[6] DIAGNOSING 'LOSS NOT DECREASING' ISSUES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "Common causes and solutions:\n",
    "\n",
    "1. LEARNING RATE TOO HIGH\n",
    "   - Symptom: Loss oscillates or increases\n",
    "   - Solution: Reduce LR (e.g., 0.001 → 0.0005)\n",
    "   \n",
    "2. LEARNING RATE TOO LOW\n",
    "   - Symptom: Loss decreases very slowly\n",
    "   - Solution: Increase LR (e.g., 0.0001 → 0.001)\n",
    "   \n",
    "3. GRADIENT VANISHING/EXPLODING\n",
    "   - Symptom: Loss becomes NaN or Inf\n",
    "   - Solution: Check gradient norm, increase CLIP value, use gradient clipping\n",
    "   \n",
    "4. BAD DATA\n",
    "   - Symptom: Loss plateaus at high value\n",
    "   - Solution: Check data quality, verify tokenization, ensure padding is correct\n",
    "   \n",
    "5. MODEL TOO SMALL\n",
    "   - Symptom: Slow improvement on training set\n",
    "   - Solution: Increase embed_dim, hidden_size, or num_layers\n",
    "   \n",
    "6. BATCH SIZE ISSUES\n",
    "   - Too small: Noisy gradients, slow training\n",
    "   - Too large: Memory issues, poor generalization\n",
    "   - Try: 32, 64, 128\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ========== 7. Memory & Performance Tips ==========\n",
    "\n",
    "print(\"\\n\\n[7] MEMORY & PERFORMANCE OPTIMIZATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "Memory-saving strategies:\n",
    "\n",
    "1. FILTER LONG SENTENCES\n",
    "   - Limit to max_len=50 tokens\n",
    "   - Code example:\n",
    "   \n",
    "   def filter_by_length(en_sents, de_sents, max_len=50):\n",
    "       data = [(en, de) for en, de in zip(en_sents, de_sents)\n",
    "               if len(en.split()) <= max_len and len(de.split()) <= max_len]\n",
    "       en_filtered, de_filtered = zip(*data)\n",
    "       return list(en_filtered), list(de_filtered)\n",
    "   \n",
    "   train_en, train_de = filter_by_length(train_en, train_de, max_len=50)\n",
    "\n",
    "2. REDUCE VOCAB SIZE\n",
    "   - Currently: 10,000 words\n",
    "   - Try: 5,000 or 8,000\n",
    "   - Trade-off: Less <unk> tokens vs smaller model\n",
    "\n",
    "3. REDUCE EMBEDDING/HIDDEN DIMENSION\n",
    "   - Current: embed_dim=512, hidden_size=512\n",
    "   - Try: 256 or 384\n",
    "   - Still gets decent results with lower memory\n",
    "\n",
    "4. USE GRADIENT ACCUMULATION (if needed)\n",
    "   - Simulate larger batch size with smaller batches\n",
    "   \n",
    "5. MIXED PRECISION (if using CUDA)\n",
    "   - Use torch.cuda.amp for faster computation\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ========== 8. Monitoring Checklist ==========\n",
    "\n",
    "print(\"\\n\\n[8] TRAINING MONITORING CHECKLIST\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "checklist = {\n",
    "    \"Tensor shapes\": \"✓ Verify in [1]\",\n",
    "    \"Data stats\": \"✓ Check in [2]\",\n",
    "    \"Gradient flow\": \"✓ Monitor in [3]\",\n",
    "    \"Teacher forcing\": \"✓ Review in [4]\",\n",
    "    \"Overfitting\": \"✓ Assess in [5]\",\n",
    "    \"Learning rate\": \"Adjust based on loss curve\",\n",
    "    \"Loss trend\": \"Should decrease monotonically (with fluctuations)\",\n",
    "    \"Validation loss\": \"Should decrease, gap with train loss < 0.5\",\n",
    "    \"Checkpoints\": \"Save best model (already doing)\",\n",
    "    \"Early stopping\": \"Patience=3 (already enabled)\",\n",
    "}\n",
    "\n",
    "for item, status in checklist.items():\n",
    "    print(f\"  ☐ {item:30s} - {status}\")\n",
    "\n",
    "\n",
    "# ========== 9. Quick Debugging Code ==========\n",
    "\n",
    "print(\"\\n\\n[9] QUICK DEBUG: Run this if loss gets stuck\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "debug_code = \"\"\"\n",
    "# Step 1: Check a single batch\n",
    "src, src_lengths, trg, trg_lengths = next(iter(train_loader))\n",
    "print(\"Input shapes OK:\", src.shape, trg.shape)\n",
    "\n",
    "# Step 2: Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(src.to(device), src_lengths.to(device), trg.to(device))\n",
    "    print(\"Output shape OK:\", out.shape)\n",
    "\n",
    "# Step 3: Compute loss manually\n",
    "pred = out[:, 1:, :].contiguous().view(-1, len(vocab_de))\n",
    "target = trg[:, 1:].contiguous().view(-1)\n",
    "loss = criterion(pred, target)\n",
    "print(\"Loss OK:\", loss.item())\n",
    "\n",
    "# Step 4: Check for NaN/Inf\n",
    "print(\"Contains NaN:\", torch.isnan(out).any().item())\n",
    "print(\"Contains Inf:\", torch.isinf(out).any().item())\n",
    "\"\"\"\n",
    "\n",
    "print(debug_code)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"END TROUBLESHOOTING GUIDE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
